{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.architecture import MLP, MLPManual\n",
    "from scripts.train import *\n",
    "from scripts.plot_utils import plot_loss_accuracy, plotValAccuracy, fillSubplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Parity Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# doesn't perform and transformation until we call the loader\n",
    "trainset = torchvision.datasets.MNIST(root='data', train=True, download=True, transform=transforms)\n",
    "testset = torchvision.datasets.MNIST(root='data', train=False, download=True, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.05\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "loss_type = \"Binary Cross Entropy\"\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "B_initialization = \"uniform\"\n",
    "optim = \"SGD\"\n",
    "momentum, nesterov_momentum = False, False\n",
    "weight_decay = 1e-3\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "model = MLP(k, \"ReLU\", loss_type)\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=learn_rate, weight_decay = weight_decay)\n",
    "\n",
    "trainLostList_Ada1, trainAccList_Ada1, valLossList_Ada1, valAccList_Ada1  = train_model(model, k, trainset, testset, loss_type, loss_fn, optimizer, num_epochs, \n",
    "                                                                                        batch_size, validate_model = True, performance=accuracy, device=device, \n",
    "                                                                                        lr_scheduler=None, updateWManually=False)\n",
    "\n",
    "plot_loss_accuracy(trainLostList_Ada1,valLossList_Ada1,trainAccList_Ada1,valAccList_Ada1,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MLP(k, \"ReLU\", loss_type)\n",
    "optimizer = torch.optim.SGD(model2.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
    "\n",
    "trainLostList_sgd1, trainAccList_sgd1, valLossList_sgd1, valAccList_sgd1  = train_model(model2, k, trainset, testset, loss_type, loss_fn, optimizer, num_epochs, \n",
    "                                                                                        batch_size, validate_model = True, performance=accuracy, device=device,lr = learn_rate, \n",
    "                                                                                        lr_scheduler=None, updateWManually=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k=1\n",
    "modelManual = MLPManual(k, learn_rate, loss_type, \"BP\", None, optim, device, False)\n",
    "\n",
    "trainLostList_sgd1_scratch, trainAccList_sgd1_scratch, \\\n",
    "valLossList_sgd1_scratch, valAccList_sgd1_scratch  = train_model_manually(modelManual, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                         nesterov_momentum, weight_decay, validate_model = True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelManualDFA = MLPManual(k, learn_rate, loss_type, \"DFA\", B_initialization, optim, device, False)\n",
    "trainLostList_sgd1_dfa, trainAccList_sgd1_dfa, \\\n",
    "valLossList_sgd1_dfa, valAccList_sgd1_dfa  =  train_model_manually(modelManualDFA, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                         nesterov_momentum, weight_decay, validate_model = True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### For k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "model3 = MLP(k,\"ReLU\", loss_type)\n",
    "optimizer = torch.optim.Adadelta(model3.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
    "\n",
    "trainLostList_Ada3, trainAccList_Ada3, \\\n",
    "valLossList_Ada3, valAccList_Ada3  = train_model(model3, k, trainset, testset, loss_type, loss_fn, optimizer, num_epochs, batch_size, validate_model = True, \n",
    "                                                 performance=accuracy, device=device, lr_scheduler=None)\n",
    "\n",
    "plot_loss_accuracy(trainLostList_Ada3,valLossList_Ada3,trainAccList_Ada3,valAccList_Ada3,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = MLP(k, \"ReLU\", loss_type)\n",
    "optimizer = torch.optim.SGD(model4.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
    "\n",
    "trainLostList_sgd3, trainAccList_sgd3, valLossList_sgd3, valAccList_sgd3  = train_model(model4, k, trainset, testset, loss_type, loss_fn, optimizer, num_epochs, \n",
    "                                                                                        batch_size, validate_model = True, performance=accuracy, device=device, \n",
    "                                                                                        lr_scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelManual3 = MLPManual(k, learn_rate, loss_type, \"BP\", None, optim, device, False)\n",
    "trainLostList_sgd3_scratch, trainAccList_sgd3_scratch, \\\n",
    "valLossList_sgd3_scratch, valAccList_sgd3_scratch  = train_model_manually(modelManual3, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                         nesterov_momentum, weight_decay, validate_model = True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.02 # one of the best lr that I got for uniform B, with 0.05 training didn't perform well\n",
    "modelManual3DFA = MLPManual(k, learn_rate, loss_type, \"DFA\", B_initialization, optim, device, False)\n",
    "trainLostList_sgd3_dfa, trainAccList_sgd3_dfa, \\\n",
    "valLossList_sgd3_dfa, valAccList_sgd3_dfa  = train_model_manually(modelManual3DFA, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                         nesterov_momentum, weight_decay, validate_model = True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax3) = plt.subplots(1, 2, figsize=(16,8))\n",
    "\n",
    "ax1.plot(range(1,21),valAccList_sgd1, color = \"blue\", label = \"SGD BP Pytorch\") \n",
    "ax1.plot(range(1,21),valAccList_Ada1, color = \"green\", label = \"Adadelta BP Pytorch\")\n",
    "ax1.plot(range(1,21),valAccList_sgd1_scratch, color = \"orange\", label = \"SGD BP Dogan\") \n",
    "ax1.plot(range(1,21),valAccList_sgd1_dfa, color = \"red\", label = \"SGD DFA Dogan\")\n",
    "ax1.set_ylim(0.40,1.05)\n",
    "ax1.set_title(\"Test Accuracy k=1\")\n",
    "ax1.set_xlabel(\"Iteration\")\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.set_xticks(range(1,21))\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "\n",
    "ax3.plot(range(1,21),valAccList_sgd3, color = \"blue\", label = \"SGD BP Pytorch\")\n",
    "ax3.plot(range(1,21),valAccList_Ada3, color = \"green\", label = \"Adadelta BP Pytorch\")\n",
    "ax3.plot(range(1,21),valAccList_sgd3_scratch, color = \"orange\", label = \"SGD BP Dogan\")\n",
    "ax3.plot(range(1,21),valAccList_sgd3_dfa, color = \"red\", label = \"SGD DFA Dogan\")\n",
    "ax3.set_ylim(0.40,1.05)\n",
    "ax3.set_title(\"Test Accuracy k=3\")\n",
    "ax3.set_xlabel(\"Iteration\")\n",
    "ax3.set_ylabel(\"Accuracy\")\n",
    "ax3.set_xticks(range(1,21))\n",
    "ax3.legend()\n",
    "ax3.grid(True)\n",
    "\n",
    "fig.savefig(\"plots/k13_SGD_Ada_BP_DFA.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with the same weights (SGD BP Pytorch vs SGD BP Dogan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "modelx = MLP(k, \"ReLU\", loss_type).to(device)\n",
    "\n",
    "w1 = copy.deepcopy(modelx.state_dict()[\"layer1.weight\"]).to(device)\n",
    "w2 = copy.deepcopy(modelx.state_dict()[\"layer2.weight\"]).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(modelx.parameters(), lr=learn_rate)\n",
    "\n",
    "trainLostList_sgd3_w, trainAccList_sgd3_w, valLossList_sgd3_w, valAccList_sgd3_w  = train_model(modelx, k, trainset, testset, loss_type, loss_fn, optimizer, num_epochs, \n",
    "                                                                                                batch_size, validate_model = True, performance=accuracy, device=device, \n",
    "                                                                                                lr=learn_rate, lr_scheduler=None, updateWManually=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "modelManualx = MLPManual(k, learn_rate, loss_type, \"BP\", None, optim, device, (w1.t(),w2.t()))\n",
    "trainLostList_sgd3_scratch_w, trainAccList_sgd3_scratch_w, \\\n",
    "valLossList_sgd3_scratch_w, valAccList_sgd3_scratch_w  = train_model_manually(modelManualx, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                         nesterov_momentum, weight_decay, validate_model = True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(range(1,21),valAccList_sgd3_w, color = \"blue\", label = \"BP SGD Pytorch\")\n",
    "plt.plot(range(1,21),valAccList_sgd3_scratch_w, color = \"green\", label = \"BP SGD Dogan\")\n",
    "\n",
    "plt.ylim(0.4,1.05)\n",
    "plt.title(\"Test Accuracy k=3\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\"plots/k3_SGD_BP_sameWeights.png\")\n",
    "\n",
    "plt.show();\n",
    "\n",
    "# They are gonna be different, because I recreate the data every epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFA Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DFA up to 100 epochs to see if we get similar result as BP\n",
    "k=3\n",
    "learn_rate = 0.01\n",
    "modelManual4 = MLPManual(k, learn_rate, loss_type, \"DFA\", B_initialization, optim, device, False)\n",
    "trainLostList_sgad4_scratch, trainAccList_sgd4_scratch, \\\n",
    "valLossList_sgd4_scratch, valAccList_sgd4_scratch  = train_model_manually(modelManual4, k, trainset, testset, loss_type, loss_fn, 100, batch_size, momentum,\n",
    "                                                                         nesterov_momentum, weight_decay, validate_model = True, device=device)\n",
    "plt.figure(figsize=(16,8))                                                                          \n",
    "plotValAccuracy(valAccList_sgd4_scratch, 100, \"DFA Validation\", 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuneLearningRate_Torch(lr_array : np.array, optim: str, k:int, loss_type):\n",
    "    listofValAcc = []\n",
    "    for learning_rate in lr_array:\n",
    "        print(f\"Learning rate: {learn_rate}\")\n",
    "        model = MLP(k, \"BP\", loss_type)\n",
    "        if optim == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, weight_decay=weight_decay)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "\n",
    "        trainLostListLoc, trainAccListLoc, valLossListLoc, valAccListLoc  = train_model(model, k, trainset, testset, loss_type, loss_fn, optimizer, num_epochs, \n",
    "                                                                                        batch_size, validate_model = True, performance=accuracy, device=device, \n",
    "                                                                                        lr_scheduler=None, updateWManually=False)\n",
    "\n",
    "        last5 = valAccListLoc[-10:]\n",
    "        meanOfLast5 = sum(last5) / len(last5)\n",
    "        listofValAcc.append(meanOfLast5)\n",
    "        listofValAccnp = np.array(listofValAcc)\n",
    "        idx = np.argsort(listofValAccnp)\n",
    "        best_lr = lr_array[idx][-1]\n",
    "    \n",
    "    print(\"Best learning rate is: \", best_lr)\n",
    "    return best_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 3\n",
    "# torch_bp_sgd_lr_array_ = np.linspace(0.05, 0.15, 6)\n",
    "# torch_bp_ada_lr_array = np.linspace(0.10, 0.20, 6)\n",
    "\n",
    "# best_torch_bp_sgd = tuneLearningRate_Torch(torch_bp_sgd_lr_array_, \"SGD\", k, loss_type)\n",
    "# best_torch_bp_ada = tuneLearningRate_Torch(torch_bp_ada_lr_array, \"Adadelta\", k, loss_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_torch_bp_sgd = 0.11\n",
    "best_torch_bp_ada = 0.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuneLearningRate_Manual(lr_array : np.array, training_method: str, init_B: str, optim: str, k:int, loss_type):\n",
    "    listofValAcc = []\n",
    "    for learning_rate in lr_array:\n",
    "        print(f\"Learning rate: {learn_rate}\")\n",
    "        model = MLPManual(k, learning_rate, loss_type, training_method, init_B, optim, device)\n",
    "        trainLostListLoc, trainAccListLoc, valLossListLoc, valAccListLoc = train_model_manually(model, k, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                                                batch_size, momentum, nesterov_momentum, weight_decay,\n",
    "                                                                                                validate_model = True, device=device)\n",
    "        last5 = valAccListLoc[-10:]\n",
    "        meanOfLast5 = sum(last5) / len(last5)\n",
    "        listofValAcc.append(meanOfLast5)\n",
    "        listofValAccnp = np.array(listofValAcc)\n",
    "        idx = np.argsort(listofValAccnp)\n",
    "        best_lr = lr_array[idx][-1]\n",
    "    \n",
    "    print(\"Best learning rate is: \", best_lr)\n",
    "    return best_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual_bp_sgd_lr_array = np.linspace(0.01, 0.07, 6)\n",
    "# manual_bp_adagrad_lr_array = np.linspace(0.005, 0.01, 6)\n",
    "# manual_bp_adadelta_lr_array = np.linspace(0.1, 1, 6)\n",
    "# manual_bp_rmsprop_lr_array = np.linspace(0.0001, 0.0005, 6)\n",
    "# manual_bp_adam_lr_array = np.linspace(0.001, 0.005, 6)\n",
    "\n",
    "\n",
    "# best_manual_bp_sgd = tuneLearningRate_Manual(manual_bp_sgd_lr_array, \"BP\", None, \"SGD\", k, loss_type)\n",
    "# best_manual_bp_adagrad = tuneLearningRate_Manual(manual_bp_adagrad_lr_array, \"BP\", None, \"Adagrad\", k, loss_type)\n",
    "# best_manual_bp_adadelta = tuneLearningRate_Manual(manual_bp_adadelta_lr_array, \"BP\", None, \"Adadelta\", k, loss_type)\n",
    "# best_manual_bp_rmsprop = tuneLearningRate_Manual(manual_bp_rmsprop_lr_array, \"BP\", None, \"RMSProp\", k, loss_type)\n",
    "# best_manual_bp_adam = tuneLearningRate_Manual(manual_bp_adam_lr_array, \"BP\", None, \"Adam\", k, loss_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_manual_bp_sgd = 0.08\n",
    "best_manual_bp_adagrad = 0.007\n",
    "best_manual_bp_adadelta = 1\n",
    "best_manual_bp_rmsprop = 0.00026\n",
    "best_manual_bp_adam = 0.0042"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune their learning rates to get best one, it is done by checking the last 10 val Accuracy\n",
    "# k=3\n",
    "# lr_array_uni = np.linspace(0.01, 0.025, 6)\n",
    "# lr_array_std_uni = np.linspace(0.0015, 0.0035, 6)\n",
    "# lr_array_gauss = np.linspace(0.01, 0.02, 6)\n",
    "# lr_array_std_gauss = np.linspace(0.0005, 0.001, 6)\n",
    "\n",
    "# best_manual_dfa_sgd_uni = tuneLearningRate(lr_array_uni, \"DFA\", \"uniform\", optim, k, loss_type)\n",
    "# best_manual_dfa_sgd_std_uni = tuneLearningRate(lr_array_std_uni, \"DFA\", \"standard uniform\", optim, k, loss_type)\n",
    "# best_manual_dfa_sgd_gaussian = tuneLearningRate(lr_array_gauss, \"DFA\", \"gaussian\", optim, k, loss_type)\n",
    "# best_manual_dfa_sgd_std_gaussian = tuneLearningRate(lr_array_std_gauss, \"DFA\", \"standard gaussian\", optim, k, loss_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning these parameters take too much time, so let's run once and store them statically\n",
    "best_manual_dfa_sgd_uni = 0.02\n",
    "best_manual_dfa_sgd_std_uni = 0.0031\n",
    "best_manual_dfa_sgd_gaussian = 0.012\n",
    "best_manual_dfa_sgd_std_gaussian = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune their learning rates to get best one, it is done by checking the last 10 val Accuracy\n",
    "# k=3\n",
    "\n",
    "# lr_array_adagrad = np.linspace(0.001, 0.005, 6)\n",
    "# lr_array_adadelta = np.linspace(0.1, 1, 6)\n",
    "# lr_array_rmsprop = np.linspace(0.0004, 0.0006, 6)\n",
    "# lr_array_adam = np.linspace(0.001, 0.003, 6)\n",
    "\n",
    "# best_manual_dfa_adagrad_uni = tuneLearningRate_Manual(lr_array_adagrad, \"DFA\", B_initialization, \"Adagrad\", k, loss_type)\n",
    "# best_manual_dfa_adadelta_uni = tuneLearningRate_Manual(lr_array_adadelta, \"DFA\", B_initialization, \"Adadelta\", k, loss_type)    \n",
    "# best_manual_dfa_rmsprop_uni = tuneLearningRate_Manual(lr_array_rmsprop, \"DFA\", B_initialization, \"RMSProp\", k, loss_type)\n",
    "# best_manual_dfa_adam_uni = tuneLearningRate_Manual(lr_array_adam, \"DFA\", B_initialization, \"Adam\", k, loss_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_manual_dfa_adagrad_uni = 0.004\n",
    "best_manual_dfa_adadelta_uni = 1\n",
    "best_manual_dfa_rmsprop_uni = 0.0004\n",
    "best_manual_dfa_adam_uni = 0.0023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Matrix Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a plot with different B initialization methods with 20 epochs with their best lr\n",
    "initializations = [\"standard uniform\", \"uniform\", \"standard gaussian\", \"gaussian\", \"BP\"]\n",
    "num_epochs=20\n",
    "K=3\n",
    "optim = \"SGD\"\n",
    "\n",
    "fig = plt.figure(figsize=(15,9))\n",
    "for ini in initializations:\n",
    "    if ini == \"uniform\":\n",
    "        modelManualx = MLPManual(K, best_manual_dfa_sgd_uni, loss_type, \"DFA\", ini, optim, device, False)\n",
    "    elif ini == \"standard uniform\":\n",
    "        modelManualx = MLPManual(K, best_manual_dfa_sgd_std_uni, loss_type, \"DFA\", ini, optim, device, False)\n",
    "    elif ini == \"gaussian\":\n",
    "        modelManualx = MLPManual(K, best_manual_dfa_sgd_gaussian, loss_type, \"DFA\", ini, optim, device, False)\n",
    "    elif ini == \"standard gaussian\":\n",
    "        modelManualx = MLPManual(K, best_manual_dfa_sgd_std_gaussian, loss_type, \"DFA\", ini, optim, device, False)\n",
    "    else:\n",
    "        modelManualx = MLPManual(K, best_manual_bp_sgd, loss_type, \"BP\", None, optim, device, False)\n",
    "    trainLostList, trainAccList, \\\n",
    "    valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay,\n",
    "                                                            validate_model = True, device=device)\n",
    "    plotValAccuracy(valAccList, num_epochs, ini, K)\n",
    "\n",
    "plt.savefig(\"plots/k3_SGD_DFA_best_randomBInit.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have plot for different learning rates (different lr_array for each random matrix)\n",
    "num_epochs = 20\n",
    "K = 3\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(16,8))\n",
    "lr_array_uni = np.linspace(0.01, 0.03, 3)\n",
    "lr_array_std_uni = np.linspace(0.0015, 0.0035, 3)\n",
    "lr_array_gauss = np.linspace(0.01, 0.02, 3)\n",
    "lr_array_std_gauss = np.linspace(0.0005, 0.0015, 3)\n",
    "\n",
    "for init in [\"standard uniform\", \"uniform\", \"standard gaussian\", \"gaussian\"]:\n",
    "    if init == \"standard uniform\":\n",
    "        for lr in lr_array_std_uni:\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay,\n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax1, init)\n",
    "    elif init == \"uniform\":\n",
    "        for lr in lr_array_uni:\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay,\n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax2, init)\n",
    "    elif init == \"standard gaussian\":\n",
    "        for lr in lr_array_std_gauss:\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay,\n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax3, init)\n",
    "    elif init == \"gaussian\":\n",
    "        for lr in lr_array_gauss:\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay,\n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax4, init)\n",
    "\n",
    "plt.savefig(\"plots/k3_SGD_DFA_BInitsWDifferentlrs.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a plot with different B initialization methods with 20 epochs with their best lr\n",
    "optims = [\"SGD\", \"Adagrad\", \"Adadelta\", \"RMSProp\", \"Adam\"]\n",
    "num_epochs=20\n",
    "K=3\n",
    "\n",
    "fig = plt.figure(figsize=(15,9))\n",
    "for optim in optims:\n",
    "    if optim == \"SGD\":\n",
    "        modelManualx = MLPManual(K, best_manual_dfa_sgd_uni, loss_type, \"DFA\", B_initialization, optim, device, False)\n",
    "    elif optim == \"Adagrad\":\n",
    "        modelManualx = MLPManual(K, best_manual_dfa_adagrad_uni, loss_type, \"DFA\", B_initialization, optim, device, False)\n",
    "    elif optim == \"Adadelta\":\n",
    "        modelManualx = MLPManual(K, best_manual_dfa_adadelta_uni, loss_type, \"DFA\", B_initialization, optim, device, False)\n",
    "    elif optim == \"RMSProp\":\n",
    "        modelManualx = MLPManual(K, best_manual_dfa_rmsprop_uni, loss_type, \"DFA\", B_initialization, optim, device, False)\n",
    "    elif optim == \"Adam\":\n",
    "        modelManualx = MLPManual(K, best_manual_dfa_adam_uni, loss_type, \"DFA\", B_initialization, optim, device, False)\n",
    "\n",
    "    trainLostList, trainAccList, \\\n",
    "    valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay,\n",
    "                                                            validate_model = True, device=device)\n",
    "    plotValAccuracy(valAccList, num_epochs, optim, K)\n",
    "\n",
    "plt.savefig(\"plots/k3_All_DFA_bestoptims.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run here again\n",
    "# have plot for different learning rates (different lr_array for each random matrix)\n",
    "num_epochs = 20\n",
    "init = \"uniform\"\n",
    "K = 3\n",
    "fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(16,8))\n",
    "lr_array_sgd = np.linspace(0.015, 0.035, 3)\n",
    "lr_array_adagrad = np.linspace(0.001, 0.005, 3)\n",
    "lr_array_adadelta = np.linspace(0.9, 1.1, 3)\n",
    "lr_array_rmsprop = np.linspace(0.0001, 0.0005, 3)\n",
    "lr_array_adam = np.linspace(0.001, 0.004, 3)\n",
    "\n",
    "for optim in [\"SGD\", \"Adagrad\", \"Adadelta\", \"RMSProp\", \"Adam\"]:\n",
    "    if optim == \"SGD\":\n",
    "        for lr in lr_array_sgd:\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay,\n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax1, optim)\n",
    "\n",
    "    elif optim == \"Adagrad\":\n",
    "        for lr in lr_array_adagrad:\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay,\n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax2, optim)\n",
    "    elif optim == \"Adadelta\":\n",
    "        for lr in lr_array_adadelta:\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay,\n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax3, optim)\n",
    "    elif optim == \"RMSProp\":\n",
    "        for lr in lr_array_rmsprop:\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay,\n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax4, optim)\n",
    "    else:\n",
    "        for lr in lr_array_adam:\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay,\n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax5, optim)\n",
    "\n",
    "\n",
    "plt.savefig(\"plots/k3_All_DFA_optimsWDifferentlrs.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce the experiments from Learning Parities with Neural Networks\n",
    "learn_rate = 0.05\n",
    "num_epochs = 20\n",
    "loss_type = \"Binary Cross Entropy\"\n",
    "fig, (ax1, ax3) = plt.subplots(1, 2, figsize=(16,8))\n",
    "for K in [1,3]:\n",
    "    for activation in [\"Adadelta\", \"NTK\", \"Gaussian features\", \"ReLU features\", \"SGD\"]:\n",
    "        model = MLP(K, activation, loss_type)\n",
    "        if \"features\" in activation:\n",
    "            # deactivate the first layer\n",
    "            optimizer = torch.optim.Adadelta(model.layer2.parameters(), lr = learn_rate, weight_decay=weight_decay)\n",
    "        elif \"NTK\" in activation:\n",
    "            paramsToUpdate = list(model.layer1.parameters()) + list(model.layer2.parameters())\n",
    "            optimizer = torch.optim.Adadelta(paramsToUpdate, lr = learn_rate, weight_decay=weight_decay)\n",
    "        elif \"SGD\" in activation:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr = best_torch_bp_sgd, weight_decay=weight_decay)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adadelta(model.parameters(), lr = best_torch_bp_ada, weight_decay=weight_decay)\n",
    "\n",
    "        print(\"Activation:\",activation)\n",
    "\n",
    "        trainLostList, trainAccList, valLossList, valAccList  = train_model(model, K, trainset, testset, loss_type, loss_fn, optimizer, num_epochs,\n",
    "                                                                            batch_size, validate_model = True, performance=accuracy,\n",
    "                                                                            device=\"cuda:0\", lr_scheduler=None)\n",
    "\n",
    "        if K == 1:\n",
    "            fillSubplot(valAccList, num_epochs, activation, ax1, \"k = \" + str(K))\n",
    "        else:\n",
    "            fillSubplot(valAccList, num_epochs, activation, ax3, \"k = \" + str(K))\n",
    "\n",
    "fig.supxlabel('Epoch')\n",
    "fig.supylabel('Test Accuracy')\n",
    "fig.savefig(\"plots/k13_SGD_ada_BP_reproduced.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "K = 3\n",
    "num_epochs = 20\n",
    "loss_type = \"Binary Cross Entropy\"\n",
    "\n",
    "fig = plt.figure(figsize=(15,9))\n",
    "for activation in [\"BP SGD\", \"DFA SGD\", \"Adadelta BP\", \"Adadelta DFA\"]:\n",
    "    #print(\"Activation:\",activation)\n",
    "    if activation == \"BP SGD\":\n",
    "        modelManual3 = MLPManual(K, best_manual_bp_sgd, loss_type, \"BP\", None, \"SGD\", device, False)\n",
    "\n",
    "    elif activation == \"DFA SGD\":\n",
    "        modelManual3 = MLPManual(K, best_manual_dfa_sgd_uni, loss_type, \"DFA\", \"uniform\", \"SGD\", device, False)\n",
    "\n",
    "    elif activation == \"Adadelta BP\":\n",
    "        modelManual3 = MLPManual(K, best_manual_bp_adadelta, loss_type, \"BP\", None, \"Adadelta\", device, False)\n",
    "\n",
    "    else: # Adadelta DFA\n",
    "        modelManual3 = MLPManual(K, best_manual_dfa_adadelta_uni, loss_type, \"DFA\", \"uniform\", \"Adadelta\", device, False)\n",
    "\n",
    "    trainLostList, trainAccList, valLossList, valAccList  = train_model_manually(modelManual3, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                                batch_size, momentum, nesterov_momentum, weight_decay,\n",
    "                                                                                validate_model = True, device=device)\n",
    "\n",
    "\n",
    "    plotValAccuracy(valAccList, num_epochs, activation, K)\n",
    "\n",
    "fig.savefig(\"plots/k3_best_SGD_delta_BP_DFA.png\")\n",
    "plt.show()\n",
    "dataset = MNISTParity(trainset, K, 128)\n",
    "dataset.plotRandomData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare BP and DFA with Adaptive methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeanStd(model):\n",
    "    results = {}\n",
    "    for i in range(1,4):\n",
    "        trainLostListLoc, trainAccListLoc, valLossListLoc, valAccListLoc = train_model_manually(model, k, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                                                batch_size, momentum, nesterov_momentum, weight_decay,\n",
    "                                                                                                validate_model = True, device=device)\n",
    "        results[i] = valAccListLoc\n",
    "        \n",
    "    liste = []\n",
    "    for i in results:\n",
    "        liste.append(results[i][-1])\n",
    "\n",
    "    return liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run here again\n",
    "k=3\n",
    "model_bp_sgd = MLPManual(k, best_manual_bp_sgd, loss_type, \"BP\", None, \"SGD\", device)\n",
    "model_bp_adagrad = MLPManual(k, best_manual_bp_adagrad, loss_type, \"BP\", None, \"Adagrad\", device)\n",
    "model_bp_rmsprop = MLPManual(k, best_manual_bp_rmsprop, loss_type, \"BP\", None, \"RMSProp\", device)\n",
    "model_bp_adadelta = MLPManual(k, best_manual_bp_adadelta, loss_type, \"BP\", None, \"Adadelta\", device)\n",
    "\n",
    "model_dfa_sgd = MLPManual(k, best_manual_dfa_sgd_uni, loss_type, \"DFA\", B_initialization, \"SGD\", device)\n",
    "model_dfa_adagrad = MLPManual(k, best_manual_dfa_adagrad_uni, loss_type, \"DFA\", B_initialization, \"Adagrad\", device)\n",
    "model_dfa_rmsprop = MLPManual(k, best_manual_dfa_rmsprop_uni, loss_type, \"DFA\", B_initialization, \"RMSProp\", device)\n",
    "model_dfa_adadelta = MLPManual(k, best_manual_dfa_adadelta_uni, loss_type, \"DFA\", B_initialization, \"Adadelta\", device)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns = [\"Train_Method\", \"Optimizer\", \"Results\"])\n",
    "idx = 0\n",
    "for model in [model_bp_sgd, model_bp_adagrad, model_bp_rmsprop, model_bp_adadelta, model_dfa_sgd, model_dfa_adagrad, model_dfa_rmsprop, model_dfa_adadelta]:\n",
    "    liste = getMeanStd(model)\n",
    "    train_Method = model.train_method\n",
    "    optim = model.optim\n",
    "    for value in liste:\n",
    "        df.loc[idx,:] = [train_Method,optim,value]\n",
    "        idx += 1\n",
    "\n",
    "df.to_csv(\"run.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "ax = sns.barplot(x=\"Optimizer\", y=\"Results\", hue=\"Train_Method\", ci=\"sd\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(\n",
    "    data=df, kind=\"bar\",\n",
    "    x=\"Optimizer\", y=\"Results\", hue=\"Train_Method\",\n",
    "    ci=\"sd\", palette=\"dark\", alpha=.6, height=10\n",
    ")\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"\", \"Test Accuracy\")\n",
    "g.legend.set_title(\"BP vs DFA with Adaptive Methods\")\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sahibinden Veri Cekme Full.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c684a6fc7ea5d844d0888e3fc402a914f9a0757a714c6fe2ccce21fe8443d9ca"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "423.991px",
    "width": "239.29px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
