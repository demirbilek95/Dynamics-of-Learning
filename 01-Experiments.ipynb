{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.architecture import MLP, MLPManual\n",
    "from scripts.train import *\n",
    "from scripts.plot_utils import plot_loss_accuracy, plotValAccuracy, fillSubplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Parity Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't perform and transformation until we call the loader\n",
    "trainset = torchvision.datasets.MNIST(root='data', train=True, download=True, transform=transforms)\n",
    "testset = torchvision.datasets.MNIST(root='data', train=False, download=True, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.05\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "loss_type = \"Binary Cross Entropy\"\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "B_initialization = \"uniform\"\n",
    "optim = \"SGD\"\n",
    "momentum, nesterov_momentum = False, False\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "model = MLP(k, \"ReLU\", loss_type)\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=learn_rate, weight_decay = 0.001)\n",
    "\n",
    "trainLostList_Ada1, trainAccList_Ada1, valLossList_Ada1, valAccList_Ada1  = train_model(model, k, trainset, testset, loss_type, loss_fn, optimizer, num_epochs, batch_size, validate_model = True, performance=accuracy, device=device, lr_scheduler=None, updateWManually=False)\n",
    "\n",
    "plot_loss_accuracy(trainLostList_Ada1,valLossList_Ada1,trainAccList_Ada1,valAccList_Ada1,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MLP(k, \"ReLU\", loss_type)\n",
    "optimizer = torch.optim.SGD(model2.parameters(), lr=learn_rate, weight_decay=0.001)\n",
    "\n",
    "trainLostList_sgd1, trainAccList_sgd1, valLossList_sgd1, valAccList_sgd1  = train_model(model2, k, trainset, testset, loss_type, loss_fn, optimizer, num_epochs, batch_size, validate_model = True, performance=accuracy, device=device,lr = learn_rate, lr_scheduler=None, updateWManually=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "modelManual = MLPManual(k, learn_rate, loss_type, \"BP\", None, optim, device, False)\n",
    "\n",
    "trainLostList_sgd1_scratch, trainAccList_sgd1_scratch, \\\n",
    "valLossList_sgd1_scratch, valAccList_sgd1_scratch  = train_model_manually(modelManual, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                         nesterov_momentum, validate_model = True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelManualDFA = MLPManual(k, learn_rate, loss_type, \"DFA\", B_initialization, optim, device, False)\n",
    "trainLostList_sgd1_dfa, trainAccList_sgd1_dfa, \\\n",
    "valLossList_sgd1_dfa, valAccList_sgd1_dfa  =  train_model_manually(modelManualDFA, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                         nesterov_momentum, validate_model = True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.ylim(0.5,1)\n",
    "plt.plot(valAccList_sgd1, label=\"SGD\")\n",
    "plt.plot(valAccList_Ada1, label=\"Adadelta\")\n",
    "plt.plot(valAccList_sgd1_scratch, label= \"SGD BP Scratch\")\n",
    "plt.plot(valAccList_sgd1_dfa, label= \"SGD DFA Scratch\")\n",
    "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### For k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "model3 = MLP(k,\"ReLU\", loss_type)\n",
    "optimizer = torch.optim.Adadelta(model3.parameters(), lr=learn_rate, weight_decay=0.001)\n",
    "\n",
    "trainLostList_Ada3, trainAccList_Ada3, \\\n",
    "valLossList_Ada3, valAccList_Ada3  = train_model(model3, k, trainset, testset, loss_type, loss_fn, optimizer, num_epochs, batch_size, validate_model = True, performance=accuracy, device=device, lr_scheduler=None)\n",
    "\n",
    "plot_loss_accuracy(trainLostList_Ada3,valLossList_Ada3,trainAccList_Ada3,valAccList_Ada3,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = MLP(k, \"ReLU\", loss_type)\n",
    "optimizer = torch.optim.SGD(model4.parameters(), lr=learn_rate, weight_decay=0.001)\n",
    "\n",
    "trainLostList_sgd3, trainAccList_sgd3, valLossList_sgd3, valAccList_sgd3  = train_model(model4, k, trainset, testset, loss_type, loss_fn, optimizer, num_epochs, batch_size, validate_model = True, performance=accuracy, device=device, lr_scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelManual3 = MLPManual(k, learn_rate, loss_type, \"BP\", None, optim, device, False)\n",
    "trainLostList_sgd3_scratch, trainAccList_sgd3_scratch, \\\n",
    "valLossList_sgd3_scratch, valAccList_sgd3_scratch  = train_model_manually(modelManual3, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                         nesterov_momentum, validate_model = True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.02 # one of the best lr that I got for uniform B, with 0.05 training didn't perform well\n",
    "modelManual3DFA = MLPManual(k, learn_rate, loss_type, \"DFA\", B_initialization, optim, device, False)\n",
    "trainLostList_sgd3_dfa, trainAccList_sgd3_dfa, \\\n",
    "valLossList_sgd3_dfa, valAccList_sgd3_dfa  = train_model_manually(modelManual3DFA, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                         nesterov_momentum, validate_model = True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.ylim(0.4,1)\n",
    "plt.plot(valAccList_sgd3, label=\"SGD\")\n",
    "plt.plot(valAccList_Ada3, label=\"Adadelta\")\n",
    "plt.plot(valAccList_sgd3_scratch, label= \"SGD BP Scratch\")\n",
    "plt.plot(valAccList_sgd3_dfa, label= \"SGD DFA Scratch\")\n",
    "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
    "plt.grid(True)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax3) = plt.subplots(1, 2, figsize=(16,8))\n",
    "\n",
    "ax1.plot(range(1,21),valAccList_sgd1, color = \"blue\", label = \"SGD BP Pytorch\") \n",
    "ax1.plot(range(1,21),valAccList_Ada1, color = \"green\", label = \"Adadelta BP Pytorch\")\n",
    "ax1.plot(range(1,21),valAccList_sgd1_scratch, color = \"orange\", label = \"SGD BP Pytorch\") \n",
    "ax1.plot(range(1,21),valAccList_sgd1_dfa, color = \"red\", label = \"SGD DFA Pytorch\")\n",
    "ax1.set_ylim(0.40,1.05)\n",
    "ax1.set_title(\"Test Accuracy k=1\")\n",
    "ax1.set_xlabel(\"Iteration\")\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.set_xticks(range(1,21))\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "\n",
    "ax3.plot(range(1,21),valAccList_sgd3, color = \"blue\", label = \"SGD BP Pytorch\")\n",
    "ax3.plot(range(1,21),valAccList_Ada3, color = \"green\", label = \"Adadelta BP Pytorch\")\n",
    "ax3.plot(range(1,21),valAccList_sgd3_scratch, color = \"orange\", label = \"SGD BP Dogan\")\n",
    "ax3.plot(range(1,21),valAccList_sgd3_dfa, color = \"red\", label = \"SGD DFA Pytorch\")\n",
    "ax3.set_ylim(0.40,1.05)\n",
    "ax3.set_title(\"Test Accuracy k=3\")\n",
    "ax3.set_xlabel(\"Iteration\")\n",
    "ax3.set_ylabel(\"Accuracy\")\n",
    "ax3.set_xticks(range(1,21))\n",
    "ax3.legend()\n",
    "ax3.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with the same weights (SGD BP Pytorch vs SGD BP Dogan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "modelx = MLP(k, \"ReLU\", loss_type).to(device)\n",
    "\n",
    "w1 = copy.deepcopy(modelx.state_dict()[\"layer1.weight\"]).to(device)\n",
    "w2 = copy.deepcopy(modelx.state_dict()[\"layer2.weight\"]).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(modelx.parameters(), lr=learn_rate)\n",
    "\n",
    "trainLostList_sgd3_w, trainAccList_sgd3_w, valLossList_sgd3_w, valAccList_sgd3_w  = train_model(modelx, k, trainset, testset, loss_type, loss_fn, optimizer, num_epochs, batch_size, validate_model = True, performance=accuracy, device=device, lr=learn_rate, lr_scheduler=None, updateWManually=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "modelManualx = MLPManual(k, learn_rate, loss_type, \"BP\", None, optim, device, (w1.t(),w2.t()))\n",
    "trainLostList_sgd3_scratch_w, trainAccList_sgd3_scratch_w, \\\n",
    "valLossList_sgd3_scratch_w, valAccList_sgd3_scratch_w  = train_model_manually(modelManualx, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                         nesterov_momentum, validate_model = True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(range(1,21),valAccList_sgd3_w, color = \"blue\", label = \"BP SGD Pytorch\")\n",
    "plt.plot(range(1,21),valAccList_sgd3_scratch_w, color = \"green\", label = \"BP SGD Dogan\")\n",
    "\n",
    "plt.ylim(0.4,1.05)\n",
    "plt.title(\"Test Accuracy k=3\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\"plots/doganVSPytorch.png\")\n",
    "\n",
    "plt.show();\n",
    "\n",
    "# They are gonna be different, because I recreate the data every epoch\n",
    "# Even without recreating, results are different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFA Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DFA up to 100 epochs to see if we get similar result as BP\n",
    "k=3\n",
    "learn_rate = 0.01\n",
    "modelManual4 = MLPManual(k, learn_rate, loss_type, \"DFA\", B_initialization, optim, device, False)\n",
    "trainLostList_sgad4_sacratch, trainAccList_sgd4_scratch, \\\n",
    "valLossList_sgd4_scratch, valAccList_sgd4_scratch  = train_model_manually(modelManual4, k, trainset, testset, loss_type, loss_fn, 100, batch_size, momentum,\n",
    "                                                                         nesterov_momentum, weight_decay, validate_model = True, device=device)\n",
    "plt.figure(figsize=(16,8))                                                                          \n",
    "plotValAccuracy(valAccList_sgd4_scratch, 100, \"DFA Validation\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have plot for same learning rates (same lr array for each random matrix)\n",
    "num_epochs = 20\n",
    "K = 3\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(16,8))\n",
    "lr_rates = np.linspace(0.001, 0.05, 5)\n",
    "for init in [\"standard uniform\", \"uniform\", \"standard gaussian\", \"gaussian\"]:\n",
    "    for lr in lr_rates:\n",
    "        if init == \"standard uniform\":\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                          batch_size, momentum, nesterov_momentum, \n",
    "                                                                          validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax1, init)\n",
    "        elif init == \"uniform\":\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                          batch_size, momentum, nesterov_momentum, \n",
    "                                                                          validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax2, init)\n",
    "        elif init == \"standard gaussian\":\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                          batch_size, momentum, nesterov_momentum, \n",
    "                                                                          validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax3, init)\n",
    "        elif init == \"gaussian\":\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                          batch_size, momentum, nesterov_momentum, \n",
    "                                                                          validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax4, init)\n",
    "\n",
    "plt.savefig(\"plots/InitsWSamelrs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuneLearningRate(lr_array : np.array, training_method: str, init_B: str, optim: str, k:int, loss_type):\n",
    "    listofValAcc = []\n",
    "    for learning_rate in lr_array:\n",
    "        model = MLPManual(k, learning_rate, loss_type, training_method, init_B, optim, device)\n",
    "        trainLostListLoc, trainAccListLoc, valLossListLoc, valAccListLoc = train_model_manually(model, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                                                batch_size, momentum, nesterov_momentum, \n",
    "                                                                                                validate_model = True, device=device)\n",
    "        last5 = valAccListLoc[-10:]\n",
    "        meanOfLast5 = sum(last5) / len(last5)\n",
    "        listofValAcc.append(meanOfLast5)\n",
    "        listofValAccnp = np.array(listofValAcc)\n",
    "        idx = np.argsort(listofValAccnp)\n",
    "        best_lr = lr_array[idx][-1]\n",
    "    \n",
    "    print(\"Best learning rate is: \", best_lr)\n",
    "    return best_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune their learning rates to get best one, it is done by checking the last 10 val Accuracy\n",
    "k=3\n",
    "lr_array_uni = np.linspace(0.01, 0.025, 6)\n",
    "lr_array_std_uni = np.linspace(0.0015, 0.0035, 6)\n",
    "lr_array_gauss = np.linspace(0.01, 0.02, 6)\n",
    "lr_array_std_gauss = np.linspace(0.0005, 0.001, 6)\n",
    "lr_array_bp = np.linspace(0.05, 0.3, 6)\n",
    "\n",
    "best_lr_uni = tuneLearningRate(lr_array_uni, \"DFA\", \"uniform\", optim, k, loss_type)\n",
    "best_lr_std_uni = tuneLearningRate(lr_array_std_uni, \"DFA\", \"standard uniform\", optim, k, loss_type)\n",
    "best_lr_gaussian = tuneLearningRate(lr_array_gauss, \"DFA\", \"gaussian\", optim, k, loss_type)\n",
    "best_lr_std_gaussian = tuneLearningRate(lr_array_std_gauss, \"DFA\", \"standard gaussian\", optim, k, loss_type)\n",
    "best_lr_bp = tuneLearningRate(lr_array_bp, \"BP\", None, optim, k, loss_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning these parameters take too much time, so let's run once and store them statically\n",
    "best_lr_uni = 0.022\n",
    "best_lr_std_uni = 0.003 \n",
    "best_lr_gaussian = 0.016\n",
    "best_lr_std_gaussian = 0.001\n",
    "best_lr_bp = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune their learning rates to get best one, it is done by checking the last 10 val Accuracy\n",
    "k=3\n",
    "lr_array_uni = np.linspace(0.01, 0.025, 6)\n",
    "lr_array_std_uni = np.linspace(0.0015, 0.0035, 6)\n",
    "lr_array_gauss = np.linspace(0.01, 0.02, 6)\n",
    "lr_array_std_gauss = np.linspace(0.001, 0.0002, 6)\n",
    "lr_array_bp = np.linspace(0.05, 0.3, 6)\n",
    "\n",
    "best_lr_adagrad = tuneLearningRate(lr_array_uni, \"DFA\", \"uniform\", optim, k, loss_type)\n",
    "best_lr_rmsprop = tuneLearningRate(lr_array_std_uni, \"DFA\", \"uniform\", optim, k, loss_type)\n",
    "best_lr_adam = tuneLearningRate(lr_array_gauss, \"DFA\", \"gaussian\", k, optim, loss_type)\n",
    "best_lr_std_gaussian = tuneLearningRate(lr_array_std_gauss, \"DFA\", \"standard gaussian\", optim, k, loss_type)\n",
    "best_lr_bp = tuneLearningRate(lr_array_bp, \"BP\", None, k, loss_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# have a plot with different B initialization methods with 20 epochs with their best lr\n",
    "initializations = [\"standard uniform\", \"uniform\", \"standard gaussian\", \"gaussian\", \"BP\"]\n",
    "num_epochs=20\n",
    "K=3\n",
    "optim = \"SGD\"\n",
    "\n",
    "fig = plt.figure(figsize=(15,9))\n",
    "for ini in initializations:\n",
    "    print(ini)\n",
    "    if ini == \"uniform\":\n",
    "        modelManualx = MLPManual(K, best_lr_uni, loss_type, \"DFA\", ini, optim, device, False)\n",
    "    elif ini == \"standard uniform\":\n",
    "        modelManualx = MLPManual(K, best_lr_std_uni, loss_type, \"DFA\", ini, optim, device, False)\n",
    "    elif ini == \"gaussian\":\n",
    "        modelManualx = MLPManual(K, best_lr_gaussian, loss_type, \"DFA\", ini, optim, device, False)\n",
    "    elif ini == \"standard gaussian\":\n",
    "        modelManualx = MLPManual(K, best_lr_std_gaussian, loss_type, \"DFA\", ini, optim, device, False)\n",
    "    else:\n",
    "        modelManualx = MLPManual(K, best_lr_bp, loss_type, \"BP\", None, optim, device, False)\n",
    "    trainLostList, trainAccList, \\\n",
    "    valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay, \n",
    "                                                            validate_model = True, device=device)\n",
    "    plotValAccuracy(valAccList, num_epochs, ini, K)\n",
    "\n",
    "plt.savefig(\"plots/randomBInit.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have plot for different learning rates (different lr_array for each random matrix)\n",
    "num_epochs = 20\n",
    "K = 3\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(16,8))\n",
    "lr_array_uni = np.linspace(0.01, 0.025, 6)\n",
    "lr_array_std_uni = np.linspace(0.0015, 0.0035, 6)\n",
    "lr_array_gauss = np.linspace(0.01, 0.02, 6)\n",
    "lr_array_std_gauss = np.linspace(0.001, 0.0002, 6)\n",
    "lr_array_bp = np.linspace(0.05, 0.3, 6)\n",
    "\n",
    "for init in [\"standard uniform\", \"uniform\", \"standard gaussian\", \"gaussian\"]:\n",
    "    if init == \"standard uniform\":\n",
    "        for lr in lr_array_std_uni:\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, \n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax1, init)\n",
    "    elif init == \"uniform\":\n",
    "        for lr in lr_array_uni:\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, \n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax2, init)\n",
    "    elif init == \"standard gaussian\":\n",
    "        for lr in lr_array_std_gauss:\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, \n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax3, init)\n",
    "    elif init == \"gaussian\":\n",
    "        for lr in lr_array_gauss:\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay, \n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax4, init)\n",
    "\n",
    "plt.savefig(\"plots/InitsWDifferentlrs.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have plot for same learning rates (same lr array for each random matrix)\n",
    "init = \"uniform\"\n",
    "num_epochs = 20\n",
    "K = 3\n",
    "fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(16,8))\n",
    "lr_rates = np.linspace(0.005, 0.025, 6)\n",
    "for optim in [\"SGD\", \"Adagrad\", \"Adadelta\", \"RMSProp\", \"Adam\"]:\n",
    "    for lr in lr_rates:\n",
    "        if optim == \"SGD\":\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, \n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax1, optim)\n",
    "        elif optim == \"Adagrad\":\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, \n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax2, optim)\n",
    "        elif optim == \"Adadelta\":\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, \n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax3, optim)\n",
    "        elif optim == \"RMSProp\":\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, \n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax4, optim)\n",
    "        else: # ADAM\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, \n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax5, optim)\n",
    "\n",
    "\n",
    "plt.savefig(\"plots/OptimsWSamelrs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune their learning rates to get best one, it is done by checking the last 10 val Accuracy\n",
    "k=3\n",
    "lr_array_adagrad = np.linspace(0.001, 0.005, 6)\n",
    "lr_array_adadelta = np.linspace(0.1, 1, 6)\n",
    "lr_array_rmsprop = np.linspace(0.0001, 0.0003, 6)\n",
    "lr_array_adam = np.linspace(0.0001, 0.001, 6)\n",
    "\n",
    "best_lr_adagrad = tuneLearningRate(lr_array_adagrad, \"DFA\", init, \"Adagrad\", k, loss_type)\n",
    "best_lr_adadelta = tuneLearningRate(lr_array_adadelta, \"DFA\", init, \"Adadelta\", k, loss_type)\n",
    "best_lr_rmsprop = tuneLearningRate(lr_array_rmsprop, \"DFA\", init, \"RMSProp\", k, loss_type)\n",
    "best_lr_adam = tuneLearningRate(lr_array_adam, \"DFA\", init, \"Adam\", k, loss_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr_SGD = 0.02\n",
    "best_lr_adagrad = 0.0042\n",
    "best_lr_adadelta = 1\n",
    "best_lr_rmsprop = 0.0001\n",
    "best_lr_adam = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a plot with different B initialization methods with 20 epochs with their best lr\n",
    "optims = [\"SGD\", \"SGD Momentum\", \"SGD Nesterov\", \"Adagrad\", \"Adadelta\", \"RMSProp\", \"Adam\"]\n",
    "num_epochs=20\n",
    "K=3\n",
    "init = \"uniform\"\n",
    "\n",
    "fig = plt.figure(figsize=(15,9))\n",
    "for optim in optims:\n",
    "    if optim == \"SGD\":\n",
    "        modelManualx = MLPManual(K, best_lr_SGD, loss_type, \"DFA\", init, optim, device, False)\n",
    "    elif optim == \"Adagrad\":\n",
    "        modelManualx = MLPManual(K, best_lr_adagrad, loss_type, \"DFA\", init, optim, device, False)\n",
    "    elif optim == \"Adadelta\":\n",
    "        modelManualx = MLPManual(K, best_lr_adadelta, loss_type, \"DFA\", init, optim, device, False)\n",
    "    elif optim == \"RMSPRop\":\n",
    "        modelManualx = MLPManual(K, best_lr_rmsprop, loss_type, \"DFA\", init, optim, device, False)\n",
    "    elif optim == \"Adam\":\n",
    "        modelManualx = MLPManual(K, best_lr_adam, loss_type, \"DFA\", init, optim, device, False)\n",
    "    else:\n",
    "        modelManualx = MLPManual(K, best_lr_adam, loss_type, \"BP\", None, optim, device, False)\n",
    "\n",
    "    trainLostList, trainAccList, \\\n",
    "    valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay, \n",
    "                                                            validate_model = True, device=device)\n",
    "    plotValAccuracy(valAccList, num_epochs, ini, K)\n",
    "\n",
    "plt.savefig(\"plots/randomBOptims.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have plot for different learning rates (different lr_array for each random matrix)\n",
    "num_epochs = 20\n",
    "init = \"uniform\"\n",
    "K = 3\n",
    "fig, (ax1, ax2, ax3, ax4, ax5, ax6, ax7) = plt.subplots(1, 7, figsize=(16,8))\n",
    "lr_array_sgd = np.linspace(0.015, 0.035, 6)\n",
    "lr_array_adagrad = np.linspace(0.001, 0.005, 6)\n",
    "lr_array_adadelta = np.linspace(0.1, 1, 6)\n",
    "lr_array_rmsprop = np.linspace(0.0001, 0.0003, 6)\n",
    "lr_array_adam = np.linspace(0.001, 0.005, 6)\n",
    "\n",
    "for optim in [\"SGD\", \"SGD Momentum\", \"SGD Nesterov\", \"Adagrad\", \"Adadelta\", \"RMSProp\", \"Adam\"]:\n",
    "    if optim == \"SGD\":\n",
    "        for lr in lr_array_sgd:\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, \n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax1, optim)\n",
    "\n",
    "    elif \"Momentum\" in optim:\n",
    "        for lr in lr_array_sgd:\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, \"SGD\", device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, 0.9, nesterov_momentum, \n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax2, optim)\n",
    "    \n",
    "    elif \"Nesterov\" in optim:\n",
    "        for lr in lr_array_sgd:\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, \"SGD\", device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, 0.9, \n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax3, optim)\n",
    "\n",
    "    elif optim == \"Adagrad\":\n",
    "        for lr in lr_array_adagrad:\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, \n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax4, optim)\n",
    "    elif optim == \"Adadelta\":\n",
    "        for lr in lr_array_adadelta:\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, \n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax5, optim)\n",
    "    elif optim == \"RMSProp\":\n",
    "        for lr in lr_array_rmsprop:\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, \n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax6, optim)\n",
    "    else:\n",
    "        for lr in lr_array_adam:\n",
    "            modelManualx = MLPManual(K, lr, loss_type, \"DFA\", init, optim, device, False)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, \n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax7, optim)\n",
    "\n",
    "\n",
    "plt.savefig(\"plots/OptimsWDifferentlrs.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Add Lazy methods\n",
    "learn_rate = 0.05\n",
    "K = 3\n",
    "num_epochs = 20\n",
    "loss_type = \"Binary Cross Entropy\"\n",
    "\n",
    "fig = plt.figure(figsize=(15,9))\n",
    "for activation in [\"Adadelta\", \"NTK\", \"Gaussian features\", \"ReLU features\", \"Linear features\", \"SGD\", \"SGD Dogan\", \"DFA\"]:\n",
    "    if activation != \"SGD Dogan\" and activation != \"DFA\":\n",
    "        model = MLP(K, activation, loss_type)\n",
    "        if \"features\" in activation:\n",
    "            # deactivate the first layer\n",
    "            optimizer = torch.optim.Adadelta(model.layer2.parameters(), lr = learn_rate, weight_decay=0.001)\n",
    "        elif \"NTK\" in activation:\n",
    "            paramsToUpdate = list(model.layer1.parameters()) + list(model.layer2.parameters())\n",
    "            optimizer = torch.optim.Adadelta(paramsToUpdate, lr = learn_rate, weight_decay=0.001)\n",
    "        elif \"SGD\" in activation:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr = learn_rate, weight_decay=0.001)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adadelta(model.parameters(), lr = learn_rate, weight_decay=0.001)\n",
    "\n",
    "        print(\"Activation:\",activation)\n",
    "\n",
    "        trainLostList, trainAccList, valLossList, valAccList  = train_model(model, K, trainset, testset, loss_type, loss_fn, optimizer, num_epochs,\n",
    "                                                                            batch_size, validate_model = True, performance=accuracy,\n",
    "                                                                            device=\"cuda:0\", lr_scheduler=None)\n",
    "    else:\n",
    "        #print(\"Activation:\",activation)\n",
    "        if activation == \"SGD Dogan\":\n",
    "            modelManual3 = MLPManual(K, learn_rate, loss_type, \"BP\", None, \"SGD\", device, False)\n",
    "\n",
    "        else:\n",
    "            learn_rate_dfa = 0.02\n",
    "            modelManual3 = MLPManual(K, learn_rate_dfa, loss_type, activation, init, optim, device, False)\n",
    "\n",
    "        trainLostList, trainAccList, valLossList, valAccList  = train_model_manually(modelManual3, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                                    batch_size, momentum, nesterov_momentum, \n",
    "                                                                                    validate_model = True, device=device)\n",
    "\n",
    "\n",
    "    plotValAccuracy(valAccList, num_epochs, activation, K)\n",
    "\n",
    "fig.savefig(\"plots/\" + str(K) + \"valAccuracy.png\")\n",
    "plt.show()\n",
    "dataset = MNISTParity(trainset, K, 128)\n",
    "dataset.plotRandomData()\n",
    "\n",
    "# TODO: Tune all the models including the lazy ones, for learning_rate and weight_decay"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sahibinden Veri Cekme Full.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c684a6fc7ea5d844d0888e3fc402a914f9a0757a714c6fe2ccce21fe8443d9ca"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "423.991px",
    "width": "239.29px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
