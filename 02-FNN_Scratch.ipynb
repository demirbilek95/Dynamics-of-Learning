{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## TODO:\n",
    "- Have plots Dogan SGD vs Pytorch SGD on k=1 and k=3\n",
    "- Might need to use BCE in default pytorch implementation\n",
    "- Try to switch to DFA from here and check the performance results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from scripts.train import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "print(torch.__version__)\n",
    "print(np.__version__)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.9.0\n",
      "1.20.3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Parity Data Iterator"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.1307,), (0.3081,))\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# doesn't perform and transformation until we call the loader\n",
    "trainset = torchvision.datasets.MNIST(root='data', train=True, download=True, transform=transforms)\n",
    "testset = torchvision.datasets.MNIST(root='data', train=False, download=True, transform=transforms)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/john/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MLP Scratch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "learn_rate = 0.05\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "loss_fn = torch.nn.BCELoss() # or BCELoss with sigmoid activation in last layer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class MLP_Manual(torch.nn.Module):\n",
    "    def __init__(self, k, device, batch_size, loss_type = \"Cross Entropy\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.input_dim = 28 * 28 * k\n",
    "        self.hidden_dim = 512\n",
    "        if loss_type == \"Cross Entropy\":\n",
    "            self.output_dim = 2\n",
    "        else:                           # BCE case\n",
    "            self.output_dim = 1\n",
    "        self.learning_rate = 0.001\n",
    "        self.flat = torch.nn.Flatten() # when input comes as 28x28, this'll convert to 784\n",
    "        # WEIGHTS\n",
    "        # initialize the weights as pytorch does by default --> IT DIVERGES and perform worse (90%) for k=1\n",
    "        # e.g. 784 x 512\n",
    "        self.w1 = torch.empty(self.input_dim, self.hidden_dim).to(device)\n",
    "        stdv1 = 1. / math.sqrt(self.w1.size(1))\n",
    "        self.w1.uniform_(-stdv1, +stdv1)\n",
    "        #  e.g. 512 x 1\n",
    "        self.w2 = torch.empty(self.hidden_dim, self.output_dim).to(device)\n",
    "        stdv2 = 1. / math.sqrt(self.w2.size(1))\n",
    "        self.w2.uniform_(-stdv2, +stdv2)\n",
    "\n",
    "        # BIASES\n",
    "        self.b1 = torch.empty(batch_size, self.hidden_dim).to(device)\n",
    "        self.b1.uniform_(-stdv1, stdv1)\n",
    "        self.b2 = torch.empty(batch_size, self.output_dim).to(device)\n",
    "        self.b2.uniform_(-stdv1, stdv1)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        maxes = torch.max(x, 1, keepdim=True)[0]\n",
    "        x_exp = torch.exp(x-maxes)\n",
    "        x_exp_sum = torch.sum(x_exp, 1, keepdim=True)\n",
    "        return x_exp/x_exp_sum\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "\n",
    "    @staticmethod\n",
    "    def reLU(s):\n",
    "        s[s < 0] = 0\n",
    "        return s.float()\n",
    "\n",
    "    @staticmethod\n",
    "    def reLUPrime(s):\n",
    "        s[s < 0] = 0\n",
    "        s[s > 0] = 1\n",
    "        return s.float()\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward(self, X):\n",
    "\n",
    "        X = self.flat(X)\n",
    "        # batch_size changes at the end of the apoch from 128 to 96, this spawned a problem in calculations \n",
    "        self.dynamic_batch_size =  X.shape[0]  \n",
    "        # a_k = W_k @ h_{k-1} + b_k, h_k = f(a_k) where h_0 = X and f is the non linearity, a_2 = y^\n",
    "        self.a1 = torch.matmul(X, self.w1) + self.b1[:self.dynamic_batch_size, :] # e.g. k=1 --> 128x784 @ 784x512 + 128x512 where 128 is batch_size (X.shape[1])\n",
    "        self.h1 = self.reLU(self.a1)       # f is the reLU\n",
    "        self.a2 = torch.matmul(self.h1, self.w2) + self.b2[:self.dynamic_batch_size, :]\n",
    "\n",
    "        if loss_type == \"Cross Entropy\":\n",
    "            y_hat = torch.nn.functional.softmax(self.a2, dim=1)\n",
    "        else:\n",
    "            y_hat = self.sigmoid(self.a2)\n",
    "\n",
    "        return y_hat # some loss functions handle output layer non-linearity\n",
    "\n",
    "    # Backward propagation\n",
    "    def backward(self, X, y, y_hat):\n",
    "        X = self.flat(X)\n",
    "        # gradients of W2 --> dBCE/dW2 = dE/dy^.dy^/da2. da2/dW2 = (y^ - y) h1\n",
    "        if loss_type == \"Cross Entropy\":\n",
    "            self.e = y_hat - torch.nn.functional.one_hot(y) # e - 128x2, h1.t - 512,128 for k=1\n",
    "        else:\n",
    "            self.e = y_hat - y.reshape(len(y),1) # e - 128x1, h1.t - 512,128 for k=1\n",
    "\n",
    "        self.w2_grads = torch.matmul(self.h1.t(), self.e)\n",
    "        # gradients of W1 --> dBCE/dW1 = dE/dh1 . dh1/da1 . da1/dW1\n",
    "        # where dE/dh1 = dE/dy^ . dy^/da2 . da2/dh1\n",
    "        self.dBCE_da1 = torch.matmul(self.e, self.w2.t()) * self.reLUPrime(self.a1) # e - 128x1, w2.t - 1,512 , a1 - 128,512\n",
    "        self.w1_grads = torch.matmul(X.t(), self.dBCE_da1) # x.t - 784,128, dBCE_da1 128,512\n",
    "        # gradients of b2 --> dBCE/db2 = dBCE/dy^. dy^/da2. da2/db2 = (y^-y)*1\n",
    "        self.b2_grads = self.e[:self.dynamic_batch_size, :]\n",
    "        # gradients of b1 --> dBCE/db1 = dBCE/dh1. dh1/da1. da1/db1\n",
    "        # where dBCE/dh1 = dBCE/dy^ . dy^/da2 . da2/dh1\n",
    "        self.b1_grads = self.dBCE_da1[:self.dynamic_batch_size, :]\n",
    "\n",
    "        # Implement SGD here\n",
    "        self.w1 -= self.learning_rate * self.w1_grads\n",
    "        self.w2 -= self.learning_rate * self.w2_grads\n",
    "        self.b1[:self.dynamic_batch_size, :] -= self.learning_rate * self.b1_grads\n",
    "        self.b2[:self.dynamic_batch_size, :] -= self.learning_rate * self.b2_grads\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # Forward propagation\n",
    "        y_hat = self.forward(X)\n",
    "        # Backward propagation and gradient descent\n",
    "        self.backward(X, y, y_hat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "k=1\n",
    "device=\"cpu\"\n",
    "loss_type = \"Binary Cross Entropy\"\n",
    "model = MLP_Manual(k, device,batch_size, loss_type)\n",
    "\n",
    "trainLostList, trainAccList, valLossList, valAccList  = train_model_manually(model, k, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                             batch_size, validate_model = True, device=device)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output dim 1\n",
      "Epoch 1 completed. Loss - total: 14603.5488 - average: 0.2434; Performance: 0.9544\n",
      "TESTING - loss 735.7558687776327 - performance 0.9732\n",
      "Epoch 2 completed. Loss - total: 3416.4844 - average: 0.0569; Performance: 0.9810\n",
      "TESTING - loss 597.4205151572824 - performance 0.9783\n",
      "Epoch 3 completed. Loss - total: 2536.7661 - average: 0.0423; Performance: 0.9865\n",
      "TESTING - loss 511.9709709137678 - performance 0.9818\n",
      "Epoch 4 completed. Loss - total: 2012.4348 - average: 0.0335; Performance: 0.9899\n",
      "TESTING - loss 461.00623723864555 - performance 0.9836\n"
     ]
    }
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sahibinden Veri Cekme Full.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c73b42f4307b5621c20050e2a07bd2e616b471b51e8418e2c366c1833db7b122"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "423.991px",
    "width": "239.29px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}