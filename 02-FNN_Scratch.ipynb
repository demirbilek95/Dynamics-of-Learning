{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- Add bias terms\n",
    "    * Here need to check how to update these terms\n",
    "- Have plots Dogan SGD vs Pytorch SGD on k=1 and k=3\n",
    "- Try to switch to DFA from here and check the performance results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n",
      "1.20.3\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Parity Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# doesn't perform and transformation until we call the loader\n",
    "trainset = torchvision.datasets.MNIST(root='data', train=True, download=True, transform=transforms)\n",
    "testset = torchvision.datasets.MNIST(root='data', train=False, download=True, transform=transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.05\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "loss_fn = torch.nn.BCELoss() # or BCELoss with sigmoid activation in last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reLUPrime(s):\n",
    "    s[s < 0] = 0\n",
    "    s[s > 0] = 1\n",
    "    return s.float()\n",
    "\n",
    "\n",
    "class MLP_Manual(torch.nn.Module):\n",
    "    def __init__(self, k, device, loss_type = \"Cross Entropy\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = 28 * 28 * k\n",
    "        self.hidden_dim = 512\n",
    "        if loss_type == \"Cross Entropy\":\n",
    "            self.output_dim = 2\n",
    "        else:                           # BCE case\n",
    "            self.output_dim = 1\n",
    "        self.learning_rate = 0.001\n",
    "        self.flat = torch.nn.Flatten() # when input comes as 28x28, this'll convert to 784\n",
    "        # WEIGHTS\n",
    "        # initialize the weights as pytorch does by default --> IT DIVERGES and perform worse (90%) for k=1\n",
    "        # e.g. 784 x 512\n",
    "        self.w1 = torch.zeros(self.input_dim, self.hidden_dim).to(device)\n",
    "        stdv1 = 1. / math.sqrt(self.w1.size(1))\n",
    "        self.w1.uniform_(-stdv1, +stdv1)\n",
    "        #  e.g. 512 x 1\n",
    "        self.w2 = torch.zeros(self.hidden_dim, self.output_dim).to(device)\n",
    "        stdv2 = 1. / math.sqrt(self.w2.size(1))\n",
    "        self.w2.uniform_(-stdv2, +stdv2)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        maxes = torch.max(x, 1, keepdim=True)[0]\n",
    "        x_exp = torch.exp(x-maxes)\n",
    "        x_exp_sum = torch.sum(x_exp, 1, keepdim=True)\n",
    "        return x_exp/x_exp_sum\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "\n",
    "    @staticmethod\n",
    "    def reLU(s):\n",
    "        s[s < 0] = 0\n",
    "        return s.float()\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward(self, X):\n",
    "\n",
    "        X = self.flat(X)\n",
    "        # a_k = W_k @ h_{k-1} + b_k, h_k = f(a_k) where h_0 = X and f is the non linearity, a_2 = y^\n",
    "        self.a1 = torch.matmul(X, self.w1) # e.g. k=1 --> 128x784 @ 784x512\n",
    "        self.h1 = self.reLU(self.a1)       # f is the reLU\n",
    "        self.a2 = torch.matmul(self.h1, self.w2) #\n",
    "\n",
    "        if loss_type == \"Cross Entropy\":\n",
    "            y_hat = torch.nn.functional.softmax(self.a2, dim=1)\n",
    "        else:\n",
    "            y_hat = self.sigmoid(self.a2)\n",
    "\n",
    "        return y_hat # some loss functions handle output layer non-linearity\n",
    "\n",
    "    # Backward propagation\n",
    "    def backward(self, X, y, y_hat):\n",
    "        X = self.flat(X)\n",
    "        # gradients of W2 --> dBCE/dW2 = dE/dy^.dy^/da2. da2/dW2 = (y^ - y) h1\n",
    "        if loss_type == \"Cross Entropy\":\n",
    "            self.e = y_hat - torch.nn.functional.one_hot(y) # e - 128x2, h1.t - 512,128 for k=1\n",
    "\n",
    "        else:\n",
    "            self.e = y_hat - y.reshape(len(y),1) # e - 128x1, h1.t - 512,128 for k=1\n",
    "\n",
    "        self.w2_grads = torch.matmul(self.h1.t(), self.e)\n",
    "        # gradients of W1 --> dBCE/dW1 = dE/dh1 . dh1/da1 . da1/dW1\n",
    "        # where dE/dh1 = dE/dy^ . dy^/da2 . da2/dh1\n",
    "        self.dBCE_da1 = torch.matmul(self.e, self.w2.t()) * reLUPrime(self.a1) # e - 128x1, w2.t - 1,512 , a1 - 128,512\n",
    "        self.w1_grads = torch.matmul(X.t(), self.dBCE_da1) # x.t - 784,128, dBCE_da1 128,512\n",
    "\n",
    "        # Implement SGD here\n",
    "        self.w1 -= self.learning_rate * self.w1_grads\n",
    "        self.w2 -= self.learning_rate * self.w2_grads\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # Forward propagation\n",
    "        y_hat = self.forward(X)\n",
    "        # Backward propagation and gradient descent\n",
    "        self.backward(X, y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Loss - total: 41607.6133 - average: 0.6935; Performance: 0.5040\n",
      "TESTING - loss 6903.118252754211 - performance 0.5252\n",
      "Epoch 2 completed. Loss - total: 40647.2852 - average: 0.6775; Performance: 0.5634\n",
      "TESTING - loss 6532.040953636169 - performance 0.6104\n",
      "Epoch 3 completed. Loss - total: 38255.5039 - average: 0.6376; Performance: 0.6421\n",
      "TESTING - loss 6193.752765655518 - performance 0.6685\n",
      "Epoch 4 completed. Loss - total: 37168.6758 - average: 0.6195; Performance: 0.6701\n",
      "TESTING - loss 6093.183219432831 - performance 0.6820\n",
      "Epoch 5 completed. Loss - total: 36625.1016 - average: 0.6104; Performance: 0.6771\n",
      "TESTING - loss 6129.885613918304 - performance 0.6691\n",
      "Epoch 6 completed. Loss - total: 36317.5938 - average: 0.6053; Performance: 0.6864\n",
      "TESTING - loss 5927.008748054504 - performance 0.7093\n",
      "Epoch 7 completed. Loss - total: 35990.4141 - average: 0.5998; Performance: 0.6942\n",
      "TESTING - loss 5902.620196342468 - performance 0.7111\n",
      "Epoch 8 completed. Loss - total: 35802.2266 - average: 0.5967; Performance: 0.6985\n",
      "TESTING - loss 5962.272882461548 - performance 0.7038\n",
      "Epoch 9 completed. Loss - total: 35567.6328 - average: 0.5928; Performance: 0.7036\n",
      "TESTING - loss 5930.144011974335 - performance 0.6987\n",
      "Epoch 10 completed. Loss - total: 35398.6602 - average: 0.5900; Performance: 0.7079\n",
      "TESTING - loss 5896.172404289246 - performance 0.7164\n",
      "Epoch 11 completed. Loss - total: 35328.4492 - average: 0.5888; Performance: 0.7094\n",
      "TESTING - loss 5831.2137722969055 - performance 0.7258\n",
      "Epoch 12 completed. Loss - total: 35110.6484 - average: 0.5852; Performance: 0.7153\n",
      "TESTING - loss 5798.747718334198 - performance 0.7233\n",
      "Epoch 13 completed. Loss - total: 34986.4609 - average: 0.5831; Performance: 0.7160\n",
      "TESTING - loss 5845.390141010284 - performance 0.7183\n",
      "Epoch 14 completed. Loss - total: 34930.6133 - average: 0.5822; Performance: 0.7163\n",
      "TESTING - loss 5784.238040447235 - performance 0.7216\n",
      "Epoch 15 completed. Loss - total: 34912.6562 - average: 0.5819; Performance: 0.7193\n",
      "TESTING - loss 5786.152720451355 - performance 0.7221\n",
      "Epoch 16 completed. Loss - total: 34674.0586 - average: 0.5779; Performance: 0.7225\n",
      "TESTING - loss 5716.882944107056 - performance 0.7327\n",
      "Epoch 17 completed. Loss - total: 34722.9648 - average: 0.5787; Performance: 0.7237\n",
      "TESTING - loss 5729.365766048431 - performance 0.7284\n",
      "Epoch 18 completed. Loss - total: 34538.8477 - average: 0.5756; Performance: 0.7236\n",
      "TESTING - loss 5720.514595508575 - performance 0.7299\n",
      "Epoch 19 completed. Loss - total: 34387.7266 - average: 0.5731; Performance: 0.7275\n",
      "TESTING - loss 5729.449510574341 - performance 0.7254\n",
      "Epoch 20 completed. Loss - total: 34353.2461 - average: 0.5726; Performance: 0.7306\n",
      "TESTING - loss 5716.810703277588 - performance 0.7272\n"
     ]
    }
   ],
   "source": [
    "k=3\n",
    "device=\"cpu\"\n",
    "loss_type = \"Cross Entropy\"\n",
    "model = MLP_Manual(k, device, loss_type)\n",
    "\n",
    "trainLostList, trainAccList, valLossList, valAccList  = train_model_manually(model, k, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                             batch_size, validate_model = True, device=device)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sahibinden Veri Cekme Full.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c73b42f4307b5621c20050e2a07bd2e616b471b51e8418e2c366c1833db7b122"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "423.991px",
    "width": "239.29px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}