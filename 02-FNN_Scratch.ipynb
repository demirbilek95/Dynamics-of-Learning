{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- Add bias terms\n",
    "    * Here need to check how to update these terms\n",
    "- Update the print statement, just need to write another functionn like test_model\n",
    "- Put functions into script folder\n",
    "- Have plots Dogan SGD vs Pytorch SGD on k=1 and k=3\n",
    "- Try to switch to DFA from here and check the performance results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.mnistParity import MNISTParity\n",
    "from scripts.architecture import MLP\n",
    "from scripts.train_utils import AverageMeter, accuracy\n",
    "from scripts.train import train_epoch, train_model, test_model\n",
    "from scripts.plot_utils import plot_loss_accuracy, plotValAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n",
      "1.20.3\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Parity Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# doesn't perform and transformation until we call the loader\n",
    "trainset = torchvision.datasets.MNIST(root='data', train=True, download=True, transform=transforms)\n",
    "testset = torchvision.datasets.MNIST(root='data', train=False, download=True, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.05\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "loss_fn = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Manual(torch.nn.Module):\n",
    "    def __init__(self, k, device, loss_type = \"Cross Entropy\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = 28 * 28 * k\n",
    "        self.hidden_dim = 512\n",
    "        if loss_type == \"Cross Entropy\":\n",
    "            self.output_dim = 2\n",
    "        else:                           # BCE case\n",
    "            self.output_dim = 1\n",
    "        self.learning_rate = 0.001\n",
    "        self.flat = torch.nn.Flatten() # when input comes as 28x28, this'll convert to 784\n",
    "        # WEIGHTS\n",
    "        # initialize the weights as pytorch does by default --> IT DIVERGES and perform worse (90%) for k=1\n",
    "        # e.g. 784 x 512\n",
    "        self.w1 = torch.zeros(self.input_dim, self.hidden_dim).to(device)\n",
    "        stdv1 = 1. / math.sqrt(self.w1.size(1))\n",
    "        self.w1.uniform_(-stdv1, +stdv1)\n",
    "        #  e.g. 512 x 1\n",
    "        self.w2 = torch.zeros(self.hidden_dim, self.output_dim).to(device)\n",
    "        stdv2 = 1. / math.sqrt(self.w2.size(1))\n",
    "        self.w2.uniform_(-stdv2, +stdv2)\n",
    "        \n",
    "    def softmax(self,x):    \n",
    "        maxes = torch.max(x, 1, keepdim=True)[0]\n",
    "        x_exp = torch.exp(x-maxes)\n",
    "        x_exp_sum = torch.sum(x_exp, 1, keepdim=True)\n",
    "        return x_exp/x_exp_sum\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "     \n",
    "    def reLU(self, s):\n",
    "        s[s < 0] = 0\n",
    "        return s.float()\n",
    "    \n",
    "    def reLUPrime(self, s):\n",
    "        s[s < 0] = 0\n",
    "        s[s > 0] = 1\n",
    "        return s.float()\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward(self, X):\n",
    "        \n",
    "        X = self.flat(X)\n",
    "        # a_k = W_k @ h_{k-1} + b_k, h_k = f(a_k) where h_0 = X and f is the non linearity, a_2 = y^\n",
    "        self.a1 = torch.matmul(X, self.w1) # e.g. k=1 --> 128x784 @ 784x512\n",
    "        self.h1 = self.reLU(self.a1)       # f is the reLU\n",
    "        self.a2 = torch.matmul(self.h1, self.w2) #\n",
    "        \n",
    "        if loss_type == \"Cross Entropy\":\n",
    "            y_hat = torch.nn.functional.softmax(self.a2, dim=1)\n",
    "        else:\n",
    "            y_hat = self.sigmoid(self.a2)\n",
    "            \n",
    "        return y_hat # some loss functions handle output layer non-linearity\n",
    "\n",
    "    # Backward propagation\n",
    "    def backward(self, X, y, y_hat):\n",
    "        X = self.flat(X)\n",
    "        # gradients of W2 --> dBCE/dW2 = dE/dy^.dy^/da2. da2/dW2 = (y^ - y) h1\n",
    "        if loss_type == \"Cross Entropy\":\n",
    "            self.e = y_hat - torch.nn.functional.one_hot(y) # e - 128x2, h1.t - 512,128 for k=1\n",
    "        else:\n",
    "            self.e = y_hat - y.reshape(len(y),1) # e - 128x1, h1.t - 512,128 for k=1\n",
    "            \n",
    "        self.w2_grads = torch.matmul(self.h1.t(), self.e)\n",
    "        # gradients of W1 --> dBCE/dW1 = dE/dh1 . dh1/da1 . da1/dW1\n",
    "        # where dE/dh1 = dE/dy^ . dy^/da2 . da2/dh1\n",
    "        self.dBCE_da1 = torch.matmul(self.e, self.w2.t()) * self.reLUPrime(self.a1) # e - 128x1, w2.t - 1,512 , a1 - 128,512\n",
    "        self.w1_grads = torch.matmul(X.t(), self.dBCE_da1) # x.t - 784,128, dBCE_da1 128,512\n",
    "        \n",
    "        # Implement SGD here\n",
    "        self.w1 -= self.learning_rate * self.w1_grads\n",
    "        self.w2 -= self.learning_rate * self.w2_grads\n",
    "\n",
    "    def train(self, X, y_hat):\n",
    "        # Forward propagation\n",
    "        y_hat = self.forward(X)\n",
    "        # Backward propagation and gradient descent\n",
    "        self.backward(X, y, y_hat)\n",
    "        \n",
    "        \n",
    "def predict(nn_output: torch.Tensor):\n",
    "    nn_output[nn_output > 0.5] = 1\n",
    "    nn_output[nn_output < 0.5] = 0\n",
    "    return nn_output.reshape(len(nn_output)).int()\n",
    "\n",
    "def predict2(nn_output: torch.Tensor):\n",
    "    return torch.argmax(nn_output, dim=1)\n",
    "    \n",
    "def accuracy(nn_output: torch.Tensor, ground_truth: torch.Tensor, loss_type = \"Cross Entropy\"):\n",
    "    # nn_out_classes = torch.argmax(nn_output, dim=1)\n",
    "    if loss_type == \"Cross Entropy\":\n",
    "        nn_out_classes = predict2(nn_output)\n",
    "    else:\n",
    "        nn_out_classes = predict(nn_output)\n",
    "    # nn_out_classes = predict2(nn_output)\n",
    "    # produce tensor of booleans - at which position of the nn output is the correct class located?\n",
    "    correct_items = (nn_out_classes == ground_truth)\n",
    "    # now getting the accuracy is easy, we just operate the sum of the tensor and divide it by the number of examples\n",
    "    acc = correct_items.sum().item() / nn_output.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Loss - total: 53879.7969 - average: 0.8980; Performance: 0.5134\n",
      "Epoch 2 completed. Loss - total: 40136.3555 - average: 0.6689; Performance: 0.5678\n",
      "Epoch 3 completed. Loss - total: 37329.0625 - average: 0.6222; Performance: 0.6312\n",
      "Epoch 4 completed. Loss - total: 35150.3516 - average: 0.5858; Performance: 0.6679\n",
      "Epoch 5 completed. Loss - total: 34091.0234 - average: 0.5682; Performance: 0.6832\n",
      "Epoch 6 completed. Loss - total: 33533.4023 - average: 0.5589; Performance: 0.6909\n",
      "Epoch 7 completed. Loss - total: 33035.7617 - average: 0.5506; Performance: 0.6999\n",
      "Epoch 8 completed. Loss - total: 32547.7207 - average: 0.5425; Performance: 0.7019\n",
      "Epoch 9 completed. Loss - total: 32103.4668 - average: 0.5351; Performance: 0.7091\n",
      "Epoch 10 completed. Loss - total: 31949.3457 - average: 0.5325; Performance: 0.7105\n",
      "Epoch 11 completed. Loss - total: 31541.4707 - average: 0.5257; Performance: 0.7186\n",
      "Epoch 12 completed. Loss - total: 31412.9668 - average: 0.5235; Performance: 0.7218\n",
      "Epoch 13 completed. Loss - total: 31120.9395 - average: 0.5187; Performance: 0.7232\n",
      "Epoch 14 completed. Loss - total: 30901.9570 - average: 0.5150; Performance: 0.7272\n",
      "Epoch 15 completed. Loss - total: 30692.3301 - average: 0.5115; Performance: 0.7316\n",
      "Epoch 16 completed. Loss - total: 30695.3672 - average: 0.5116; Performance: 0.7327\n",
      "Epoch 17 completed. Loss - total: 30539.5059 - average: 0.5090; Performance: 0.7329\n",
      "Epoch 18 completed. Loss - total: 30150.4492 - average: 0.5025; Performance: 0.7370\n",
      "Epoch 19 completed. Loss - total: 30220.9316 - average: 0.5037; Performance: 0.7370\n",
      "Epoch 20 completed. Loss - total: 30038.2656 - average: 0.5006; Performance: 0.7386\n"
     ]
    }
   ],
   "source": [
    "# training loop for MLP_Manual\n",
    "\n",
    "k=3\n",
    "device = \"cpu\"\n",
    "loss_type = \"Binary Cross Entropy\"\n",
    "model_a = MLP_Manual(k, device, loss_type)\n",
    "loss_fn = torch.nn.BCELoss() # or BCELoss with sigmoid activation in last layer\n",
    "\n",
    "for epoch in range(20):\n",
    "    trainData = MNISTParity(trainset, k, batch_size)\n",
    "    loss_meter = AverageMeter()\n",
    "    performance_meter = AverageMeter()\n",
    "\n",
    "    for X,y in trainData.loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        y_hat = model_a(X)\n",
    "        \n",
    "        if loss_type == \"Cross Entropy\":\n",
    "            loss = torch.nn.functional.cross_entropy(y_hat,y)\n",
    "        else:\n",
    "            loss = loss_fn(y_hat,y.reshape(len(y),1).float())\n",
    "            \n",
    "        acc = accuracy(y_hat, y, loss_type)\n",
    "        loss_meter.update(val=loss, n=X.shape[0])\n",
    "        performance_meter.update(val=acc, n=X.shape[0])\n",
    "        model_a.train(X,y)\n",
    "     \n",
    "    print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum:.4f} - average: {loss_meter.avg:.4f}; Performance: {performance_meter.avg:.4f}\")   "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sahibinden Veri Cekme Full.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c73b42f4307b5621c20050e2a07bd2e616b471b51e8418e2c366c1833db7b122"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "423.991px",
    "width": "239.29px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
