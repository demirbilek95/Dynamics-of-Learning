{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## TODO:\n",
    "- Add bias terms\n",
    "    * Here need to check how to update these terms\n",
    "- Have plots Dogan SGD vs Pytorch SGD on k=1 and k=3\n",
    "- Try to switch to DFA from here and check the performance results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from scripts.train import *"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(torch.__version__)\n",
    "print(np.__version__)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Parity Data Iterator"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.1307,), (0.3081,))\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# doesn't perform and transformation until we call the loader\n",
    "trainset = torchvision.datasets.MNIST(root='data', train=True, download=True, transform=transforms)\n",
    "testset = torchvision.datasets.MNIST(root='data', train=False, download=True, transform=transforms)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MLP Scratch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "learn_rate = 0.05\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "loss_fn = torch.nn.BCELoss() # or BCELoss with sigmoid activation in last layer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MLP_Manual(torch.nn.Module):\n",
    "    def __init__(self, k, device, loss_type = \"Cross Entropy\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = 28 * 28 * k\n",
    "        self.hidden_dim = 512\n",
    "        if loss_type == \"Cross Entropy\":\n",
    "            self.output_dim = 2\n",
    "        else:                           # BCE case\n",
    "            self.output_dim = 1\n",
    "        self.learning_rate = 0.001\n",
    "        self.flat = torch.nn.Flatten() # when input comes as 28x28, this'll convert to 784\n",
    "        # WEIGHTS\n",
    "        # initialize the weights as pytorch does by default --> IT DIVERGES and perform worse (90%) for k=1\n",
    "        # e.g. 784 x 512\n",
    "        self.w1 = torch.zeros(self.input_dim, self.hidden_dim).to(device)\n",
    "        stdv1 = 1. / math.sqrt(self.w1.size(1))\n",
    "        self.w1.uniform_(-stdv1, +stdv1)\n",
    "        #  e.g. 512 x 1\n",
    "        self.w2 = torch.zeros(self.hidden_dim, self.output_dim).to(device)\n",
    "        stdv2 = 1. / math.sqrt(self.w2.size(1))\n",
    "        self.w2.uniform_(-stdv2, +stdv2)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        maxes = torch.max(x, 1, keepdim=True)[0]\n",
    "        x_exp = torch.exp(x-maxes)\n",
    "        x_exp_sum = torch.sum(x_exp, 1, keepdim=True)\n",
    "        return x_exp/x_exp_sum\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "\n",
    "    @staticmethod\n",
    "    def reLU(s):\n",
    "        s[s < 0] = 0\n",
    "        return s.float()\n",
    "\n",
    "    @staticmethod\n",
    "    def reLUPrime(s):\n",
    "        s[s < 0] = 0\n",
    "        s[s > 0] = 1\n",
    "        return s.float()\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward(self, X):\n",
    "\n",
    "        X = self.flat(X)\n",
    "        # a_k = W_k @ h_{k-1} + b_k, h_k = f(a_k) where h_0 = X and f is the non linearity, a_2 = y^\n",
    "        self.a1 = torch.matmul(X, self.w1) # e.g. k=1 --> 128x784 @ 784x512\n",
    "        self.h1 = self.reLU(self.a1)       # f is the reLU\n",
    "        self.a2 = torch.matmul(self.h1, self.w2) #\n",
    "\n",
    "        if loss_type == \"Cross Entropy\":\n",
    "            y_hat = torch.nn.functional.softmax(self.a2, dim=1)\n",
    "        else:\n",
    "            y_hat = self.sigmoid(self.a2)\n",
    "\n",
    "        return y_hat # some loss functions handle output layer non-linearity\n",
    "\n",
    "    # Backward propagation\n",
    "    def backward(self, X, y, y_hat):\n",
    "        X = self.flat(X)\n",
    "        # gradients of W2 --> dBCE/dW2 = dE/dy^.dy^/da2. da2/dW2 = (y^ - y) h1\n",
    "        if loss_type == \"Cross Entropy\":\n",
    "            self.e = y_hat - torch.nn.functional.one_hot(y) # e - 128x2, h1.t - 512,128 for k=1\n",
    "\n",
    "        else:\n",
    "            self.e = y_hat - y.reshape(len(y),1) # e - 128x1, h1.t - 512,128 for k=1\n",
    "\n",
    "        self.w2_grads = torch.matmul(self.h1.t(), self.e)\n",
    "        # gradients of W1 --> dBCE/dW1 = dE/dh1 . dh1/da1 . da1/dW1\n",
    "        # where dE/dh1 = dE/dy^ . dy^/da2 . da2/dh1\n",
    "        self.dBCE_da1 = torch.matmul(self.e, self.w2.t()) * self.reLUPrime(self.a1) # e - 128x1, w2.t - 1,512 , a1 - 128,512\n",
    "        self.w1_grads = torch.matmul(X.t(), self.dBCE_da1) # x.t - 784,128, dBCE_da1 128,512\n",
    "\n",
    "        # Implement SGD here\n",
    "        self.w1 -= self.learning_rate * self.w1_grads\n",
    "        self.w2 -= self.learning_rate * self.w2_grads\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # Forward propagation\n",
    "        y_hat = self.forward(X)\n",
    "        # Backward propagation and gradient descent\n",
    "        self.backward(X, y, y_hat)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "k=3\n",
    "device=\"cpu\"\n",
    "loss_type = \"Cross Entropy\"\n",
    "model = MLP_Manual(k, device, loss_type)\n",
    "\n",
    "trainLostList, trainAccList, valLossList, valAccList  = train_model_manually(model, k, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                             batch_size, validate_model = True, device=device)\n"
   ],
   "outputs": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sahibinden Veri Cekme Full.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c73b42f4307b5621c20050e2a07bd2e616b471b51e8418e2c366c1833db7b122"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "423.991px",
    "width": "239.29px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}