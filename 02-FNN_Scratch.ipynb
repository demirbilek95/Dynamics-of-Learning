{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- Try to modify the network to be able to use with cross entropy (same as notebook 1)\n",
    "    * here the problem is loss becomes nan immediately, probably there is a bug in backprop, it is not clear how to propagate the error\n",
    "- Try to add bias term\n",
    "    * Here need to check how to update these terms\n",
    "- Initialize the weights properyly\n",
    "    * I did it but performance was worse\n",
    "- Try to have same performance for k=3, now with existing setup, it doesn't learn\n",
    "\n",
    "- Works for BCE\n",
    "- Try to switch to Cross entropy with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.mnistParity import MNISTParity\n",
    "from scripts.architecture import MLP\n",
    "from scripts.train_utils import accuracy\n",
    "from scripts.train import train_epoch, train_model, test_model\n",
    "from scripts.plot_utils import plot_loss_accuracy, plotValAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n",
      "1.20.3\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Parity Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# doesn't perform and transformation until we call the loader\n",
    "trainset = torchvision.datasets.MNIST(root='data', train=True, download=True, transform=transforms)\n",
    "testset = torchvision.datasets.MNIST(root='data', train=False, download=True, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.05\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "loss_fn = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bias terms --> a bit confusing now with batch_size and it's not clear how to update them at the moment, ignoring for now\n",
    "# try to have same structure with other training --> I had (2 outputs) with cross entropy loss,\n",
    "# try to have the same performance for k = 3 --> doesn't perform well at all\n",
    "\n",
    "import math\n",
    "\n",
    "class MLP_Manual(torch.nn.Module):\n",
    "    def __init__(self, k, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = 28 * 28 * k\n",
    "        self.hidden_dim = 512\n",
    "        self.output_dim = 1           # TODO: make this part 2 with cross-entropy loss\n",
    "        self.learning_rate = 0.001\n",
    "        self.flat = torch.nn.Flatten() # when input comes as 28x28, this'll convert to 784\n",
    "        # WEIGHTS\n",
    "        # initialize the weights as pytorch does by default --> IT DIVERGES and perform worse (90%) for k=1\n",
    "        # e.g. 784 x 512\n",
    "        self.w1 = torch.zeros(self.input_dim, self.hidden_dim).to(device)\n",
    "        stdv1 = 1. / math.sqrt(self.w1.size(1))\n",
    "        self.w1.uniform_(-stdv1, +stdv1)\n",
    "        #  e.g. 512 x 1\n",
    "        self.w2 = torch.zeros(self.hidden_dim, self.output_dim).to(device)\n",
    "        stdv2 = 1. / math.sqrt(self.w2.size(1))\n",
    "        self.w2.uniform_(-stdv2, +stdv2)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "     \n",
    "    def reLU(self, s):\n",
    "        s[s < 0] = 0\n",
    "        return s.float()\n",
    "    \n",
    "    def reLUPrime(self, s):\n",
    "        s[s < 0] = 0\n",
    "        s[s > 0] = 1\n",
    "        return s.float()\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward(self, X):\n",
    "        X = self.flat(X)\n",
    "        # a_k = W_k @ h_{k-1} + b_k, h_k = f(a_k) where h_0 = X and f is the non linearity, a_2 = y^\n",
    "        self.a1 = torch.matmul(X, self.w1) # e.g. k=1 --> 128x784 @ 784x512\n",
    "        self.h1 = self.reLU(self.a1)       # f is the reLU\n",
    "        self.a2 = torch.matmul(self.h1, self.w2) #\n",
    "        y_hat = self.sigmoid(self.a2)\n",
    "        return y_hat # some loss functions handle output layer non-linearity\n",
    "\n",
    "    # Backward propagation\n",
    "    def backward(self, X, y, y_hat):\n",
    "        X = self.flat(X)\n",
    "        # gradients of W2 --> dBCE/dW2 = dE/dy^.dy^/da2. da2/dW2 = (y^ - y) h1 \n",
    "        self.e = y_hat - y.reshape(len(y),1) # e - 128x1, h1.t - 512,128 for k=1\n",
    "        self.w2_grads = torch.matmul(self.h1.t(), self.e)\n",
    "        # gradients of W1 --> dBCE/dW1 = dE/dh1 . dh1/da1 . da1/dW1\n",
    "        # where dE/dh1 = dE/dy^ . dy^/da2 . da2/dh1\n",
    "        self.dBCE_da1 = torch.matmul(self.e, self.w2.t()) * self.reLUPrime(self.a1) # e - 128x1, w2.t - 1,512 , a1 - 128,512\n",
    "        self.w1_grads = torch.matmul(X.t(), self.dBCE_da1) # x.t - 784,128, dBCE_da1 128,512\n",
    "        \n",
    "        # Implement SGD here\n",
    "        self.w1 -= self.learning_rate * self.w1_grads\n",
    "        self.w2 -= self.learning_rate * self.w2_grads\n",
    "\n",
    "    def train(self, X, y_hat):\n",
    "        # Forward propagation\n",
    "        y_hat = self.forward(X)\n",
    "        # Backward propagation and gradient descent\n",
    "        self.backward(X, y, y_hat)\n",
    "        \n",
    "        \n",
    "def predict(nn_output: torch.Tensor):\n",
    "    nn_output[nn_output > 0.5] = 1\n",
    "    nn_output[nn_output < 0.5] = 0\n",
    "    return nn_output.reshape(len(nn_output)).int()\n",
    "\n",
    "def predict2(nn_output: torch.Tensor):\n",
    "    return torch.argmax(nn_output, dim=1)\n",
    "    \n",
    "def accuracy(nn_output: torch.Tensor, ground_truth: torch.Tensor):\n",
    "    # nn_out_classes = torch.argmax(nn_output, dim=1)\n",
    "    nn_out_classes = predict(nn_output)\n",
    "    # nn_out_classes = predict2(nn_output)\n",
    "    # produce tensor of booleans - at which position of the nn output is the correct class located?\n",
    "    correct_items = (nn_out_classes == ground_truth)\n",
    "    # now getting the accuracy is easy, we just operate the sum of the tensor and divide it by the number of examples\n",
    "    acc = correct_items.sum().item() / nn_output.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Loss - total: 49285.7461 - average: 0.8214; Performance: 0.5141\n",
      "Epoch 2 completed. Loss - total: 39075.3945 - average: 0.6513; Performance: 0.5927\n",
      "Epoch 3 completed. Loss - total: 36375.4375 - average: 0.6063; Performance: 0.6489\n",
      "Epoch 4 completed. Loss - total: 34783.4062 - average: 0.5797; Performance: 0.6701\n",
      "Epoch 5 completed. Loss - total: 34081.9805 - average: 0.5680; Performance: 0.6875\n",
      "Epoch 6 completed. Loss - total: 33428.3711 - average: 0.5571; Performance: 0.6904\n",
      "Epoch 7 completed. Loss - total: 33025.4414 - average: 0.5504; Performance: 0.6982\n",
      "Epoch 8 completed. Loss - total: 32518.9297 - average: 0.5420; Performance: 0.7028\n",
      "Epoch 9 completed. Loss - total: 32328.6992 - average: 0.5388; Performance: 0.7077\n",
      "Epoch 10 completed. Loss - total: 32225.4688 - average: 0.5371; Performance: 0.7103\n",
      "Epoch 11 completed. Loss - total: 31806.0059 - average: 0.5301; Performance: 0.7136\n",
      "Epoch 12 completed. Loss - total: 31628.4824 - average: 0.5271; Performance: 0.7163\n",
      "Epoch 13 completed. Loss - total: 31428.6367 - average: 0.5238; Performance: 0.7183\n",
      "Epoch 14 completed. Loss - total: 31180.7246 - average: 0.5197; Performance: 0.7199\n",
      "Epoch 15 completed. Loss - total: 30957.4980 - average: 0.5160; Performance: 0.7259\n",
      "Epoch 16 completed. Loss - total: 30858.0547 - average: 0.5143; Performance: 0.7281\n",
      "Epoch 17 completed. Loss - total: 30678.7988 - average: 0.5113; Performance: 0.7337\n",
      "Epoch 18 completed. Loss - total: 30555.2559 - average: 0.5093; Performance: 0.7299\n",
      "Epoch 19 completed. Loss - total: 30408.5469 - average: 0.5068; Performance: 0.7325\n",
      "Epoch 20 completed. Loss - total: 30238.1035 - average: 0.5040; Performance: 0.7340\n"
     ]
    }
   ],
   "source": [
    "# training loop for MLP_Manual\n",
    "\n",
    "k=3\n",
    "device = \"cuda:0\"\n",
    "model_a = MLP_Manual(k, device)\n",
    "loss_fn = torch.nn.BCELoss() # or BCELoss with sigmoid activarion in last layer\n",
    "#loss_fn = torch.nn.CrossEntropyLoss() # TODO: fix loss becomes nan\n",
    "from scripts.train_utils import AverageMeter\n",
    "\n",
    "for epoch in range(20):\n",
    "    trainData = MNISTParity(trainset, k, batch_size)\n",
    "    loss_meter = AverageMeter()\n",
    "    performance_meter = AverageMeter()\n",
    "\n",
    "    for X,y in trainData.loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = model_a(X)\n",
    "        loss = loss_fn(y_hat,y.reshape(len(y),1).float())\n",
    "        #loss = loss_fn(y_hat,y).float()\n",
    "        acc = accuracy(y_hat, y)\n",
    "        loss_meter.update(val=loss, n=X.shape[0])\n",
    "        performance_meter.update(val=acc, n=X.shape[0])\n",
    "        model_a.train(X,y)\n",
    "     \n",
    "    #print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum:.4f} - average: {loss_meter.avg:.4f}\")   \n",
    "    print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum:.4f} - average: {loss_meter.avg:.4f}; Performance: {performance_meter.avg:.4f}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropy(y, y_hat):\n",
    "    \"\"\"y : is the ground truth one-hot encoded\n",
    "       y_hat is the softmax output of the network\"\"\"\n",
    "    return -torch.sum(y * torch.log(y_hat))\n",
    "\n",
    "def BCE(y, y_hat):\n",
    "    return -torch.sum(y*torch.log(y_hat) + (1-y)*torch.log(1-y_hat))\n",
    "\n",
    "def softmax(out):\n",
    "    e = torch.exp(out)\n",
    "    return e / torch.sum(e)\n",
    "\n",
    "## MISSING ONE PART, LEFT IT LIKE THIS FOR NOW\n",
    "def softmaxPrime(out):\n",
    "    e = torch.exp(out)\n",
    "    S = e / torch.sum(e)\n",
    "    return S * (1-S)\n",
    "\n",
    "\n",
    "def sigmoid(s):\n",
    "    return 1 / (1 + torch.exp(-s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = MNISTParity(trainset, 1, 128)\n",
    "X,y = next(iter(trainData.loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE THE CROSS ENTROPY BP PROCESS IN PAPER\n",
    "## THERE IS A MISTAKE BP PROCESS\n",
    "## BE CAREFUL ABOUT NOTATION AND MATRIX SIZES\n",
    "## https://www.ics.uci.edu/~pjsadows/notes.pdf\n",
    "import math\n",
    "\n",
    "def reLU(s):\n",
    "    s[s < 0] = 0\n",
    "    return s.float()\n",
    "\n",
    "def reLUPrime(s):\n",
    "    s[s < 0] = 0\n",
    "    s[s > 1] = 1\n",
    "    return s.float()\n",
    "\n",
    "w1 = torch.rand(784, 512)\n",
    "w2 = torch.rand(512, 2)\n",
    "stdv1 = 1. / math.sqrt(w1.size(1))\n",
    "stdv2 = 1. / math.sqrt(w2.size(1))\n",
    "\n",
    "w1.uniform_(-stdv1, +stdv1)\n",
    "w2.uniform_(-stdv2, +stdv2)\n",
    "\n",
    "for i in range(5):\n",
    "    #Forward Pass\n",
    "    flat = torch.nn.Flatten()\n",
    "    X = flat(X)\n",
    "    a1 = torch.matmul(X, w1)\n",
    "    h1 = reLU(a1)\n",
    "    ay = torch.matmul(h1, w2)\n",
    "    y_hat = softmax(ay)\n",
    "    #Backward\n",
    "    e = y_hat - torch.nn.functional.one_hot(y)\n",
    "#     print(torch.max(e), torch.min(e))\n",
    "#     print(e.shape, X.shape)\n",
    "    delta_a1 = torch.matmul(e, w2.t()) * softmaxPrime(h1)\n",
    "    w1 -= 0.001 * torch.matmul(X.t(), delta_a1)\n",
    "    w2 -= 0.001 * torch.matmul(h1.t(), e)\n",
    "#   print(w1.shape)\n",
    "#   print(torch.max(y_hat), torch.min(y_hat))\n",
    "    print(crossEntropy(torch.nn.functional.one_hot(y), y_hat))\n",
    "    print(accuracy(predict2(y_hat), y))\n",
    "\n",
    "#     print(torch.max(delta_a1), torch.min(delta_a1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.softmax(ay[0],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deneme = ay[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(deneme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.nn.functional.softmax(deneme, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sahibinden Veri Cekme Full.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c73b42f4307b5621c20050e2a07bd2e616b471b51e8418e2c366c1833db7b122"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "423.991px",
    "width": "239.29px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
