{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $d\\geq 2$ is the input data dimension\n",
    "- $k$ is the number of cluster per dimensions (over $2$ dimensions)\n",
    "- $n$ is the number of training samples\n",
    "- $m$ is the number of neurons\n",
    "- $p$ is cluster labels\n",
    "- sd number of spurious dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.architecture import MLP, MLPManual\n",
    "from scripts.train_utils import AverageMeter, accuracy\n",
    "from scripts.plot_utils import plot_loss_accuracy, plotValAccuracy, fillSubplot\n",
    "from scripts.optimizer import Optimizer\n",
    "from scripts.train import *\n",
    "from scripts.data import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (16, 8),\n",
    "         'axes.labelsize': 'large',\n",
    "         'axes.titlesize':'large',\n",
    "         'xtick.labelsize':'large',\n",
    "         'ytick.labelsize':'large'}\n",
    "pylab.rcParams.update(params)\n",
    "\n",
    "matplotlib.rc('xtick', labelsize=18) \n",
    "matplotlib.rc('ytick', labelsize=18) \n",
    "matplotlib.rc('font', size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "d = 5\n",
    "sd = d - 3\n",
    "n = 1000\n",
    "n_test = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "learning_rate = 0.5\n",
    "loss_type = \"Binary Cross Entropy\"\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 1000\n",
    "optim = \"SGD\"\n",
    "measure_alignment = False\n",
    "momentum, nesterov_momentum = False, False\n",
    "weight_decay = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = syntheticData(k, n, n_test, sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelManual = MLPManual(d, learning_rate, loss_type, \"BP\", None, optim, device, measure_alignment, True, False)\n",
    "trainLostList_sgd1_scratch, trainAccList_sgd1_scratch, \\\n",
    "valLossList_sgd1_scratch, valAccList_sgd1_scratch,_,_  = train_model_manually(modelManual, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                         nesterov_momentum, weight_decay, measure_alignment, n,d, validate_model = True, device=device, data=\"synthetic\")\n",
    "\n",
    "plot_loss_accuracy(trainLostList_sgd1_scratch,valLossList_sgd1_scratch,trainAccList_sgd1_scratch,valAccList_sgd1_scratch,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelManual = MLPManual(d, learning_rate, loss_type, \"BP\", None, optim, device, measure_alignment, False, False)\n",
    "\n",
    "trainLostList_sgd1_scratch, trainAccList_sgd1_scratch, \\\n",
    "valLossList_sgd1_scratch, valAccList_sgd1_scratch,_,_  = train_model_manually(modelManual, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                              nesterov_momentum, weight_decay, measure_alignment, n,d, validate_model = True, device=device, data=\"synthetic\")\n",
    "\n",
    "plot_loss_accuracy(trainLostList_sgd1_scratch,valLossList_sgd1_scratch,trainAccList_sgd1_scratch,valAccList_sgd1_scratch,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelManual = MLPManual(d, learning_rate, loss_type, \"DFA\", \"uniform\", optim, device, measure_alignment, True, False)\n",
    "\n",
    "trainLostList_sgd1_scratch, trainAccList_sgd1_scratch, \\\n",
    "valLossList_sgd1_scratch, valAccList_sgd1_scratch,_,_  = train_model_manually(modelManual, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                              nesterov_momentum, weight_decay, measure_alignment, n,d, validate_model = True, device=device, data=\"synthetic\")\n",
    "\n",
    "plot_loss_accuracy(trainLostList_sgd1_scratch,valLossList_sgd1_scratch,trainAccList_sgd1_scratch,valAccList_sgd1_scratch,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "ns = [32,64,128,256,512]\n",
    "n_test = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please note that changing the structure of data (it changes for each run)\n",
    "# will give you different results, to reproduce the experiments we've set layer \n",
    "# size 1000\n",
    "trainset, testset = syntheticData(k, n_max, n_test, sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 15 # similar to paper\n",
    "sd = d - 3\n",
    "idx = 0\n",
    "n_max = max(ns)\n",
    "df = pd.DataFrame(columns=[\"Test Accuracy\", \"n\", \"Method\"])\n",
    "\n",
    "for i in range(1,4): \n",
    "    for n in ns:\n",
    "        print(\"Number of data points:\", n)\n",
    "        modelManual1 = MLPManual(d, learning_rate, loss_type, \"BP\", None, \"SGD\", device, measure_alignment, True, False)\n",
    "        modelManual2 = MLPManual(d, learning_rate, loss_type, \"BP\", None, \"SGD\", device, measure_alignment, False, False)\n",
    "        # modelManual1 = MLPManual(d, learning_rate, loss_type, \"BP\", None, \"Adam\", device, measure_alignment, True, False)\n",
    "        # modelManual2 = MLPManual(d, learning_rate, loss_type, \"BP\", None, \"RMSProp\", device, measure_alignment, True, False)\n",
    "        # modelManual3 = MLPManual(d, learning_rate, loss_type, \"DFA\", \"uniform\", \"Adam\", device, measure_alignment, True, False)\n",
    "        # modelManual4 = MLPManual(d, learning_rate, loss_type, \"DFA\", \"uniform\", \"RMSProp\", device, measure_alignment, True, False)\n",
    "        results = {}\n",
    "        for model in [modelManual1, modelManual2]:    \n",
    "            trainLostList_sgd1_scratch1, trainAccList_sgd1_scratch1, \\\n",
    "            valLossList_sgd1_scratch1, valAccList_sgd1_scratch1,_,_  = train_model_manually(model, k, trainset, testset, loss_type, loss_fn, num_epochs, n, momentum,\n",
    "                                                                                            nesterov_momentum, weight_decay, measure_alignment, n,d, validate_model = True, device=device,\n",
    "                                                                                            data=\"synthetic\")\n",
    "            results[i] = valAccList_sgd1_scratch1\n",
    "\n",
    "            liste = []\n",
    "            for i in results:\n",
    "                liste.append(results[i][-1])\n",
    "\n",
    "            method = model.train_method + \"_\" + model.optim + \"_\" + str(model.update_both)\n",
    "\n",
    "            for value in liste:\n",
    "                df.loc[idx,:] = [value, n, method]\n",
    "                idx += 1\n",
    "\n",
    "df[\"Error\"] = df[\"Test Accuracy\"].apply(lambda x: 1-x)\n",
    "df.to_csv(\"runs/syntheticData_n.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 256\n",
    "ds = [5,10,15,20,25]\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "df = pd.DataFrame(columns=[\"Test Accuracy\", \"d\", \"Method\"])\n",
    "sd = max(ds) - 3\n",
    "\n",
    "for i in range(1,4):  \n",
    "    for d in ds:\n",
    "        print(\"Number of dimensions points:\", d)\n",
    "        modelManual1 = MLPManual(d, learning_rate, loss_type, \"BP\", None, \"SGD\", device, measure_alignment, True, False)\n",
    "        modelManual2 = MLPManual(d, learning_rate, loss_type, \"BP\", None, \"SGD\", device, measure_alignment, False, False)\n",
    "        # modelManual3 = MLPManual(d, learning_rate, loss_type, \"DFA\", \"uniform\", \"SGD\", device, measure_alignment, True, False)\n",
    "        # modelManual4 = MLPManual(d, learning_rate, loss_type, \"DFA\", \"uniform\", \"RMSProp\", device, measure_alignment, True, False)\n",
    "        # modelManual5 = MLPManual(d, learning_rate, loss_type, \"BP\", None, \"RMSProp\", device, measure_alignment, True, False)\n",
    "            \n",
    "        results = {} \n",
    "        for model in [modelManual1, modelManual2]:\n",
    "            trainLostList_sgd1_scratch1, trainAccList_sgd1_scratch1, \\\n",
    "            valLossList_sgd1_scratch1, valAccList_sgd1_scratch1,_,_  = train_model_manually(model, k, trainset, testset, loss_type, loss_fn, num_epochs, n, momentum,\n",
    "                                                                                            nesterov_momentum, weight_decay, measure_alignment, n,d, validate_model = True, device=device,\n",
    "                                                                                            data=\"synthetic\")\n",
    "            results[i] = valAccList_sgd1_scratch1\n",
    "\n",
    "            liste = []\n",
    "            for i in results:\n",
    "                liste.append(results[i][-1])\n",
    "\n",
    "            method = model.train_method + \"_\" + model.optim + \"_\" + str(model.update_both)\n",
    "\n",
    "            for value in liste:\n",
    "                df.loc[idx,:] = [value, d, method]\n",
    "                idx += 1\n",
    "\n",
    "df[\"Error\"] = df[\"Test Accuracy\"].apply(lambda x: 1-x)\n",
    "df.to_csv(\"runs/syntheticData_d.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproduced experiment from paper\n",
    "X1 = trainset[:][0][trainset[:][1] == 1, :]\n",
    "X2 = trainset[:][0][trainset[:][1] == 0, :]\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16,8))\n",
    "ax1.plot(X1[:,1],X1[:,2],\"+r\")\n",
    "ax1.plot(X2[:,1],X2[:,2],\"ob\", markerfacecolor='none')\n",
    "ax1.plot(cluster_center(torch.arange(0,k**2+1),k)[0],cluster_center(torch.arange(0,k**2+1),k)[1],\"ok\")\n",
    "ax1.set_title(\"Distribution\")\n",
    "ax1.axis(\"equal\")\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "\n",
    "df_n = pd.read_csv(\"runs/syntheticData_n.csv\")\n",
    "df_n = df_n[df_n[\"Method\"].apply(lambda x: \"DFA\" not in x)]\n",
    "g = sns.pointplot(data=df_n, x=\"n\", y=\"Error\", hue=\"Method\", alpha=.6, ax=ax2)\n",
    "g.legend_.set_title(None)\n",
    "ax2.legend()\n",
    "ax2.set_ylabel(\"Test \\nError\", rotation=0, labelpad=30)\n",
    "ax2.yaxis.set_label_coords(-0.2,0.5)\n",
    "ax2.set_xlabel(\"n\", rotation=0, labelpad=30)\n",
    "ax2.grid()\n",
    "ax2.set_title(\"Test Error vs. $n$\")\n",
    "ax2.set_ylim(0,0.5)\n",
    "ax2.set_yticks([0.0, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "\n",
    "\n",
    "df_d = pd.read_csv(\"runs/syntheticData_d.csv\")\n",
    "df_d = df_d[df_d[\"Method\"].apply(lambda x: \"DFA\" not in x)]\n",
    "g = sns.pointplot(data=df_d, x=\"d\", y=\"Error\", hue=\"Method\", alpha=.6, ax=ax3)\n",
    "ax3.get_legend().remove()\n",
    "ax3.set_ylabel(\"\")\n",
    "ax3.set_xlabel(\"d\", rotation=0, labelpad=30)\n",
    "ax3.grid()\n",
    "ax3.set_title(\"Test error vs. $d$\")\n",
    "ax3.set_ylim(0,0.5)\n",
    "ax3.set_yticks([0.0, 0.1, 0.2, 0.3, 0.4, 0.5]);\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"plots/syntheticData_reproduced.png\")\n",
    "#fig.savefig(\"../../2-writing/oxforddown/figures/2_syntheticData_reproduced.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set layer_size = 512\n",
    "idx = 0\n",
    "df = pd.DataFrame(columns=[\"Test Accuracy\", \"d\", \"Method\"])\n",
    "sd = max(ds) - 3\n",
    "\n",
    "for i in range(1,4):  \n",
    "    for d in ds:\n",
    "        print(\"Number of dimensions points:\", d)\n",
    "        modelManual1 = MLPManual(d, learning_rate, loss_type, \"BP\", None, \"SGD\", device, measure_alignment, True, False)\n",
    "        modelManual3 = MLPManual(d, learning_rate, loss_type, \"DFA\", \"uniform\", \"SGD\", device, measure_alignment, True, False)\n",
    "        modelManual4 = MLPManual(d, learning_rate, loss_type, \"DFA\", \"uniform\", \"RMSProp\", device, measure_alignment, True, False)\n",
    "        modelManual5 = MLPManual(d, learning_rate, loss_type, \"BP\", None, \"RMSProp\", device, measure_alignment, True, False)\n",
    "            \n",
    "        results = {} \n",
    "        for model in [modelManual1, modelManual3, modelManual4, modelManual5]:\n",
    "            trainLostList_sgd1_scratch1, trainAccList_sgd1_scratch1, \\\n",
    "            valLossList_sgd1_scratch1, valAccList_sgd1_scratch1,_,_  = train_model_manually(model, k, trainset, testset, loss_type, loss_fn, num_epochs, n, momentum,\n",
    "                                                                                            nesterov_momentum, weight_decay, measure_alignment, n,d, validate_model = True, device=device,\n",
    "                                                                                            data=\"synthetic\")\n",
    "\n",
    "            results[i] = valAccList_sgd1_scratch1\n",
    "\n",
    "            liste = []\n",
    "            for i in results:\n",
    "                liste.append(results[i][-1])\n",
    "\n",
    "            method = model.train_method + \"_\" + model.optim + \"_\" + str(model.update_both)\n",
    "\n",
    "            for value in liste:\n",
    "                df.loc[idx,:] = [value, d, method]\n",
    "                idx += 1\n",
    "\n",
    "df[\"Error\"] = df[\"Test Accuracy\"].apply(lambda x: 1-x)\n",
    "df.to_csv(\"runs/syntheticData_d_adap.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_max = 512\n",
    "sd = 22\n",
    "# trainset, testset = syntheticData(k, n_max, n_test, sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax2, ax3) = plt.subplots(1, 2, figsize=(16,8))\n",
    "\n",
    "X1 = trainset[:][0][trainset[:][1] == 1, :]\n",
    "X2 = trainset[:][0][trainset[:][1] == 0, :]\n",
    "\n",
    "ax2.plot(X1[:,1],X1[:,2],\"+r\")\n",
    "ax2.plot(X2[:,1],X2[:,2],\"ob\", markerfacecolor='none')\n",
    "ax2.plot(cluster_center(torch.arange(0,k**2+1),k)[0],cluster_center(torch.arange(0,k**2+1),k)[1],\"ok\")\n",
    "ax2.set_title(\"Distribution\")\n",
    "ax2.axis(\"equal\")\n",
    "ax2.axis(\"off\")\n",
    "\n",
    "df_n = pd.read_csv(\"runs/syntheticData_d_adap.csv\")\n",
    "g = sns.pointplot(data=df_n, x=\"d\", y=\"Error\", hue=\"Method\", alpha=.6, ax=ax3)\n",
    "g.legend_.set_title(None)\n",
    "ax3.legend(loc=\"lower right\")\n",
    "ax3.set_ylabel(\"Test \\nError\", rotation=0, labelpad=30)\n",
    "ax3.yaxis.set_label_coords(-0.15,0.5)\n",
    "ax3.set_xlabel(\"d\", rotation=0, labelpad=30)\n",
    "ax3.grid()\n",
    "ax3.set_title(\"Test Error vs. $d$\")\n",
    "ax3.set_ylim(0,0.5)\n",
    "ax3.set_yticks([0.0, 0.1, 0.2, 0.3, 0.4, 0.5])\n",
    "fig.savefig(\"plots/syntheticData_adaptive.png\")\n",
    "#fig.savefig(\"../../2-writing/oxforddown/figures/3_syntheticData_adaptive.png\");"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sahibinden Veri Cekme Full.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c684a6fc7ea5d844d0888e3fc402a914f9a0757a714c6fe2ccce21fe8443d9ca"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "423.991px",
    "width": "239.29px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
