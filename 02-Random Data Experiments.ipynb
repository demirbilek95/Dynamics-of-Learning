{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $d\\geq 2$ is the input data dimension\n",
    "- $k$ is the number of cluster per dimensions (over $2$ dimensions)\n",
    "- $n$ is the number of training samples\n",
    "- $m$ is the number of neurons\n",
    "- $p$ is cluster labels\n",
    "- sd number of spurious dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.architecture import MLP, MLPManual\n",
    "from scripts.train_utils import AverageMeter, accuracy\n",
    "from scripts.plot_utils import plot_loss_accuracy, plotValAccuracy, fillSubplot\n",
    "from scripts.optimizer import Optimizer\n",
    "from scripts.train import *\n",
    "from scripts.data import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "loss_type = \"Binary Cross Entropy\"\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "num_epochs = 100\n",
    "d = 3\n",
    "sd = d - 3\n",
    "sd = 0\n",
    "n = 100\n",
    "n_test = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check why we have 3 - cluster always\n",
    "trainset, testset = randomData(k, n, n_test, sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = trainset[:][0][trainset[:][1] == 1, :]\n",
    "X2 = trainset[:][0][trainset[:][1] == 0, :]\n",
    "\n",
    "plt.figure(figsize = (16,8))\n",
    "plt.plot(X1[:,1],X1[:,2],\"+r\")\n",
    "plt.plot(X2[:,1],X2[:,2],\"_b\")\n",
    "plt.plot(cluster_center(torch.range(0,k**2),k)[0],cluster_center(torch.range(0,k**2),k)[1],\"ok\")\n",
    "plt.axis(\"equal\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"plots/randomData.png\")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = d - 3\n",
    "n = 256\n",
    "n_test = 4000\n",
    "trainset, testset = randomData(k, n, n_test, sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.5\n",
    "batch_size = 1000\n",
    "optim = \"SGD\"\n",
    "measure_alignment = False\n",
    "momentum, nesterov_momentum = False, False\n",
    "weight_decay = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelManual = MLPManual(d, learning_rate, loss_type, \"BP\", None, optim, device, measure_alignment, True, False)\n",
    "trainLostList_sgd1_scratch, trainAccList_sgd1_scratch, \\\n",
    "valLossList_sgd1_scratch, valAccList_sgd1_scratch,_,_  = train_model_manually(modelManual, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                         nesterov_momentum, weight_decay, measure_alignment, n,d, validate_model = True, device=device, data=\"random\")\n",
    "\n",
    "plot_loss_accuracy(trainLostList_sgd1_scratch,valLossList_sgd1_scratch,trainAccList_sgd1_scratch,valAccList_sgd1_scratch,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelManual = MLPManual(d, learning_rate, loss_type, \"BP\", None, optim, device, measure_alignment, False, False)\n",
    "\n",
    "trainLostList_sgd1_scratch, trainAccList_sgd1_scratch, \\\n",
    "valLossList_sgd1_scratch, valAccList_sgd1_scratch,_,_  = train_model_manually(modelManual, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                         nesterov_momentum, weight_decay, measure_alignment, n,d, validate_model = True, device=device, data=\"random\")\n",
    "\n",
    "plot_loss_accuracy(trainLostList_sgd1_scratch,valLossList_sgd1_scratch,trainAccList_sgd1_scratch,valAccList_sgd1_scratch,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure b\n",
    "k=3\n",
    "num_epochs = 250\n",
    "d = 15\n",
    "sd = d - 3\n",
    "ns = [32,64,128,256,512]\n",
    "n_test = 4000\n",
    "\n",
    "learning_rate = 0.5\n",
    "optim = \"SGD\"\n",
    "measure_alignment = False\n",
    "momentum, nesterov_momentum = False, False\n",
    "weight_decay = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = max(ns)\n",
    "trainset, testset = randomData(k, n, n_test, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "df = pd.DataFrame(columns=[\"Test Accuracy\", \"n\", \"Method\"])\n",
    "\n",
    "for n in ns:\n",
    "    print(\"Number of data points:\", n)\n",
    "    modelManual1 = MLPManual(d, learning_rate, loss_type, \"BP\", None, optim, device, measure_alignment, True, False)\n",
    "    modelManual2 = MLPManual(d, learning_rate, loss_type, \"BP\", None, optim, device, measure_alignment, False, False)\n",
    "    modelManual3 = MLPManual(d, learning_rate, loss_type, \"DFA\", \"uniform\", optim, device, measure_alignment, True, False)\n",
    "    results = {}\n",
    "    for i in range(1,2):\n",
    "        for model in [modelManual1, modelManual2, modelManual3]:        \n",
    "            trainLostList_sgd1_scratch1, trainAccList_sgd1_scratch1, \\\n",
    "            valLossList_sgd1_scratch1, valAccList_sgd1_scratch1,_,_  = train_model_manually(model, k, trainset, testset, loss_type, loss_fn, num_epochs, n, momentum,\n",
    "                                                                                    nesterov_momentum, weight_decay, measure_alignment, n,d, validate_model = True, device=device,\n",
    "                                                                                    data=\"random\")\n",
    "            results[i] = valAccList_sgd1_scratch1\n",
    "\n",
    "            liste = []\n",
    "            for i in results:\n",
    "                liste.append(results[i][-1])\n",
    "\n",
    "            if model.train_method == \"BP\":\n",
    "                if model.update_both == True:\n",
    "                    method = \"BP Both Layers\"\n",
    "                else:\n",
    "                    method = \"BP Output Layer\"\n",
    "            else:\n",
    "                method = \"DFA\"\n",
    "\n",
    "            for value in liste:\n",
    "                df.loc[idx,:] = [value, n, method]\n",
    "                idx += 1\n",
    "\n",
    "df[\"Error\"] = df[\"Test Accuracy\"].apply(lambda x: 1-x)\n",
    "df.to_csv(\"runs/randomData_n.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = pd.read_csv(\"runs/randomData_n.csv\")\n",
    "ticks = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "tickLabels = map(str, ticks)\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "g = sns.pointplot(data=df_main, x=\"n\", y=\"Error\", hue=\"Layer\", alpha=.6,)\n",
    "g.legend_.set_title(None)\n",
    "\n",
    "plt.ylabel(\"Test \\nError\", rotation=0, fontsize=12, labelpad=30)\n",
    "plt.xlabel(\"n\", rotation=0, fontsize=12, labelpad=30)\n",
    "plt.grid()\n",
    "plt.ylim(0,1)\n",
    "plt.yticks([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "plt.savefig(\"plots/randomData_n.png\")\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 256\n",
    "ds = [5,10,15,20,25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "df = pd.DataFrame(columns=[\"Test Accuracy\", \"d\", \"Method\"])\n",
    "\n",
    "for d in ds:\n",
    "    print(\"Number of dimensions points:\", d)\n",
    "    modelManual1 = MLPManual(d, learning_rate, loss_type, \"BP\", None, optim, device, measure_alignment, True, False)\n",
    "    modelManual2 = MLPManual(d, learning_rate, loss_type, \"BP\", None, optim, device, measure_alignment, False, False)\n",
    "    modelManual3 = MLPManual(d, learning_rate, loss_type, \"DFA\", \"uniform\", optim, device, measure_alignment, True, False)\n",
    "    results = {}\n",
    "    for i in range(1,2):\n",
    "        for model in [modelManual1, modelManual2, modelManual3]:        \n",
    "            trainLostList_sgd1_scratch1, trainAccList_sgd1_scratch1, \\\n",
    "            valLossList_sgd1_scratch1, valAccList_sgd1_scratch1,_,_  = train_model_manually(model, k, trainset, testset, loss_type, loss_fn, num_epochs, n, momentum,\n",
    "                                                                                    nesterov_momentum, weight_decay, measure_alignment, n,d, validate_model = True, device=device,\n",
    "                                                                                    data=\"random\")\n",
    "            results[i] = valAccList_sgd1_scratch1\n",
    "\n",
    "            liste = []\n",
    "            for i in results:\n",
    "                liste.append(results[i][-1])\n",
    "\n",
    "            if model.train_method == \"BP\":\n",
    "                if model.update_both == True:\n",
    "                    method = \"BP Both Layers\"\n",
    "                else:\n",
    "                    method = \"BP Output Layer\"\n",
    "            else:\n",
    "                method = \"DFA\"\n",
    "\n",
    "            for value in liste:\n",
    "                df.loc[idx,:] = [value, d, method]\n",
    "                idx += 1\n",
    "\n",
    "df[\"Error\"] = df[\"Test Accuracy\"].apply(lambda x: 1-x)\n",
    "df.to_csv(\"runs/randomData_d.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = pd.read_csv(\"runs/randomData_d.csv\")\n",
    "ticks = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "tickLabels = map(str, ticks)\n",
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "g = sns.pointplot(data=df_main, x=\"d\", y=\"Error\", hue=\"Method\", alpha=.6,)\n",
    "g.legend_.set_title(None)\n",
    "\n",
    "plt.ylabel(\"Test \\nError\", rotation=0, fontsize=12, labelpad=30)\n",
    "plt.xlabel(\"n\", rotation=0, fontsize=12, labelpad=30)\n",
    "plt.grid()\n",
    "plt.ylim(0,1)\n",
    "plt.yticks([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "plt.savefig(\"plots/randomData_d.png\")\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check why this problem (data) is important\n",
    "- Try to replicate some results from the paper (it'll help to validate the correctnes of the implementation)\n",
    "- Later move to comparasion between lazy method, BP and DFA (be careful about the)\n",
    "    * Run 3-5 times and plot the final accuracy for different d\n",
    "    * Plot them as in the paper\n",
    "- After having the result of DFA if there's difference try to improve it with adaptive methods.\n",
    "- Lastly refactor the code to make it suitable for random data experiments\n",
    "\n",
    "- First replicate the result from the paper, plot b and c\n",
    "- Late add the DFA"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sahibinden Veri Cekme Full.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c684a6fc7ea5d844d0888e3fc402a914f9a0757a714c6fe2ccce21fe8443d9ca"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "423.991px",
    "width": "239.29px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
