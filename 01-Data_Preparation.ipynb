{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.mnistParity import MNISTParity\n",
    "from scripts.architecture import MLP\n",
    "from scripts.train_utils import accuracy\n",
    "from scripts.train import train_epoch, train_model, test_model\n",
    "from scripts.plot_utils import plot_loss_accuracy, plotValAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Parity Data Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't perform and transformation until we call the loader\n",
    "trainset = torchvision.datasets.MNIST(root='data', train=True, download=True, transform=transforms)\n",
    "testset = torchvision.datasets.MNIST(root='data', train=False, download=True, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.05\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k=1\n",
    "model = MLP(k, \"ReLU\")\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=learn_rate, weight_decay = 0.001)\n",
    "\n",
    "trainLostList, trainAccList, valLossList, valAccList  = train_model(model, k, trainset, testset, loss_fn, optimizer, num_epochs, batch_size, validate_model = True,\n",
    "                                                                     performance=accuracy, device=\"cuda:0\", lr_scheduler=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bias terms --> a bit confusing now with batch_size and it's not clear how to update them at the moment\n",
    "# try to have same structure with other training --> I had (2 output) with cross entropy loss, it didn't perform well (50%)\n",
    "# try to have the same performance for k = 3 --> doesn't perform well at all\n",
    "# check the equations one more time (DFA paper + Sebastian's paper + internet BP blog)\n",
    "\n",
    "class MLP_Manual(torch.nn.Module):\n",
    "    def __init__(self, k, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = 28 * 28 * k\n",
    "        self.hidden_dim = 512\n",
    "        self.output_dim = 1 # I tried with 2 + cross entropy, it didn't perform well\n",
    "        self.learning_rate = 0.001\n",
    "        self.flat = torch.nn.Flatten()\n",
    "        # weights\n",
    "        # e.g. 784 x 512\n",
    "        self.w1 = torch.randn(self.input_dim, self.hidden_dim).to(device)\n",
    "        #  e.g. 512 x 1\n",
    "        self.w2 = torch.randn(self.hidden_dim, self.output_dim).to(device)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "     \n",
    "    def reLU(self, s):\n",
    "        s[s < 0] = 0\n",
    "        return s.float()\n",
    "    \n",
    "    def reLUPrime(self, s):\n",
    "        s[s < 0] = 0\n",
    "        s[s > 1] = 1\n",
    "        return s.float()\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward(self, X):\n",
    "        X = self.flat(X)\n",
    "        # First linear layer\n",
    "        self.a1 = torch.matmul(X, self.w1)\n",
    "        # First non-linearity\n",
    "        self.h1 = self.reLU(self.a1)\n",
    "        # Second linear layer\n",
    "        self.ay = torch.matmul(self.h1, self.w2)\n",
    "        # Second non-linearity\n",
    "        y_hat = self.sigmoid(self.ay)\n",
    "        return y_hat\n",
    "\n",
    "    # Backward propagation\n",
    "    def backward(self, X, y, y_hat):\n",
    "        X = self.flat(X)\n",
    "        self.e = yhat - y.reshape(len(y),1)\n",
    "\n",
    "        self.delta_a1 = torch.matmul(self.e, self.w2.t()) * self.reLUPrime(self.a1)\n",
    "            \n",
    "        # Gradient descent on the weights from our 2 linear layers\n",
    "        self.change_w1 = -self.learning_rate * torch.matmul(X.t(), self.delta_a1)\n",
    "        self.change_w2 = -self.learning_rate * torch.matmul(self.h1.t(), self.e)\n",
    "        \n",
    "        self.w1 += self.change_w1\n",
    "        self.w2 += self.change_w2\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # Forward propagation\n",
    "        y_hat = self.forward(X)\n",
    "        # Backward propagation and gradient descent\n",
    "        self.backward(X, y, y_hat)\n",
    "        \n",
    "        \n",
    "def predict(nn_output: torch.Tensor):\n",
    "    nn_output[nn_output > 0.5] = 1\n",
    "    nn_output[nn_output < 0.5] = 0\n",
    "    return nn_output.reshape(len(nn_output)).int()\n",
    "    \n",
    "def accuracy(nn_output: torch.Tensor, ground_truth: torch.Tensor):\n",
    "    nn_out_classes = predict(nn_output)\n",
    "    # produce tensor of booleans - at which position of the nn output is the correct class located?\n",
    "    correct_items = (nn_out_classes == ground_truth)\n",
    "    # now getting the accuracy is easy, we just operate the sum of the tensor and divide it by the number of examples\n",
    "    acc = correct_items.sum().item() / nn_output.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop for MLP_Manual\n",
    "\n",
    "k=1\n",
    "device = \"cuda:0\"\n",
    "model_a = MLP_Manual(k, device)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "from scripts.train_utils import AverageMeter\n",
    "\n",
    "for epoch in range(20):\n",
    "    trainData = MNISTParity(trainset, k, batch_size)\n",
    "    loss_meter = AverageMeter()\n",
    "    performance_meter = AverageMeter()\n",
    "\n",
    "    for X,y in trainData.loader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        yhat = model_a(X)\n",
    "        loss = loss_fn(yhat,y.reshape(len(y),1).float())\n",
    "        acc = accuracy(yhat, y)\n",
    "        loss_meter.update(val=loss.item(), n=X.shape[0])\n",
    "        performance_meter.update(val=acc, n=X.shape[0])\n",
    "        model_a.train(X,y)\n",
    "  \n",
    "    #print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum:.4f} - average: {loss_meter.avg:.4f}\")   \n",
    "    print(f\"Epoch {epoch+1} completed. Loss - total: {loss_meter.sum:.4f} - average: {loss_meter.avg:.4f}; Performance: {performance_meter.avg:.4f}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "model2 = MLP(k, \"ReLU\")\n",
    "optimizer = torch.optim.SGD(model2.parameters(), lr=learn_rate)\n",
    "\n",
    "trainLostList, trainAccList, valLossList, valAccList  = train_model(model2, k, trainset, testset, loss_fn, optimizer, num_epochs, batch_size, validate_model = True,\n",
    "                                                                     performance=accuracy, device=\"cuda:0\",lr = learn_rate, lr_scheduler=None, updateWManually=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(trainLostList,valLossList,trainAccList,valAccList,num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "\n",
    "model3 = MLP(k,\"ReLU\")\n",
    "optimizer = torch.optim.Adadelta(model3.parameters(), lr=learn_rate, weight_decay = 0.001)\n",
    "\n",
    "trainLostList3, trainAccList3, valLossList3, valAccList3  = train_model(model3, k, trainset, testset, loss_fn, optimizer, num_epochs, batch_size, validate_model = True,\n",
    "                                                                     performance=accuracy, device=\"cuda:0\", lr_scheduler=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(trainLostList3,valLossList3,trainAccList3,valAccList3,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "model4 = MLP(k, \"ReLU\")\n",
    "optimizer = torch.optim.SGD(model4.parameters(), lr=learn_rate, weight_decay = 0.001)\n",
    "\n",
    "trainLostList4, trainAccList4, valLossList4, valAccList4  = train_model(model4, k, trainset, testset, loss_fn, optimizer, num_epochs, batch_size, validate_model = True,\n",
    "                                                                     performance=accuracy, device=\"cuda:0\", lr_scheduler=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(trainLostList4,valLossList4,trainAccList4,valAccList4,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Lazy methods\n",
    "learn_rate = 0.05\n",
    "K = 3\n",
    "num_epochs = 20\n",
    "\n",
    "fig = plt.figure()\n",
    "for activation in [\"ReLU\", \"NTK\", \"Gaussian features\", \"ReLU features\", \"linear features\", \"SGD\"]:\n",
    "    model = MLP(K, activation)\n",
    "\n",
    "    if \"features\" in activation:\n",
    "        # deactivate the first layer\n",
    "        optimizer = torch.optim.Adadelta(model.layer2.parameters(), lr = learn_rate, weight_decay=0.001)\n",
    "    elif \"NTK\" in activation:\n",
    "        paramsToUpdate = list(model.layer1.parameters()) + list(model.layer2.parameters())\n",
    "        optimizer = torch.optim.Adadelta(paramsToUpdate, lr = learn_rate, weight_decay=0.001)\n",
    "    elif \"SGD\" in activation:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr = learn_rate, weight_decay=0.001)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adadelta(model.parameters(), lr = learn_rate, weight_decay=0.001)\n",
    "\n",
    "    print(\"Activation:\",activation)\n",
    "\n",
    "    trainLostList, trainAccList, valLossList, valAccList  = train_model(model, K, trainset, testset, loss_fn, optimizer, num_epochs, \n",
    "                                                                        batch_size, validate_model = True, performance=accuracy, \n",
    "                                                                        device=\"cuda:0\", lr_scheduler=None)\n",
    "\n",
    "    plotValAccuracy(valAccList,num_epochs, activation, K)\n",
    "\n",
    "fig.savefig(str(K) + \"valAccuracy.png\")\n",
    "plt.show()\n",
    "dataset = MNISTParity(trainset, K, 128)\n",
    "dataset.plotRandomData()\n",
    "\n",
    "# just need to find good lr and weight_decay values for lazy methods to have more similar plots to paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sahibinden Veri Cekme Full.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c73b42f4307b5621c20050e2a07bd2e616b471b51e8418e2c366c1833db7b122"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "423.991px",
    "width": "239.29px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
