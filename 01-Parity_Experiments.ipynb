{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parity experiments hidden layer size is 512, please check this from `scripts/ architecture.py``\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.architecture import MLP, MLPManual\n",
    "from scripts.train import *\n",
    "from scripts.plot_utils import plot_loss_accuracy, plotValAccuracy, fillSubplot\n",
    "from scripts.notebook_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (16, 8),\n",
    "         'axes.labelsize': 'large',\n",
    "         'axes.titlesize':'large',\n",
    "         'xtick.labelsize':'large',\n",
    "         'ytick.labelsize':'large'}\n",
    "pylab.rcParams.update(params)\n",
    "matplotlib.rc('xtick', labelsize=18) \n",
    "matplotlib.rc('ytick', labelsize=18) \n",
    "matplotlib.rc('font', size=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Parity Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# doesn't perform and transformation until we call the loader\n",
    "trainset = torchvision.datasets.MNIST(root='data', train=True, download=True, transform=transforms)\n",
    "testset = torchvision.datasets.MNIST(root='data', train=False, download=True, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.05\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "loss_type = \"Binary Cross Entropy\"\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "B_initialization = \"uniform\"\n",
    "optim = \"SGD\"\n",
    "momentum, nesterov_momentum = False, False\n",
    "weight_decay = 1e-3\n",
    "measure_alignment = False\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For k = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "model = MLP(k, \"ReLU\", loss_type)\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=learn_rate, weight_decay = weight_decay)\n",
    "\n",
    "trainLostList_Ada1, trainAccList_Ada1, valLossList_Ada1, valAccList_Ada1  = train_model(model, k, trainset, testset, loss_type, loss_fn, optimizer, num_epochs, \n",
    "                                                                                        batch_size, validate_model = True, performance=accuracy, device=device, \n",
    "                                                                                        lr_scheduler=None, updateWManually=False)\n",
    "\n",
    "plot_loss_accuracy(trainLostList_Ada1,valLossList_Ada1,trainAccList_Ada1,valAccList_Ada1,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MLP(k, \"ReLU\", loss_type)\n",
    "optimizer = torch.optim.SGD(model2.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
    "\n",
    "trainLostList_sgd1, trainAccList_sgd1, valLossList_sgd1, valAccList_sgd1  = train_model(model2, k, trainset, testset, loss_type, loss_fn, optimizer, num_epochs, \n",
    "                                                                                        batch_size, validate_model = True, performance=accuracy, device=device,lr = learn_rate, \n",
    "                                                                                        lr_scheduler=None, updateWManually=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k=1\n",
    "input_dim = 28 * 28 * k \n",
    "modelManual = MLPManual(input_dim, learn_rate, loss_type, \"BP\", None, optim, device, measure_alignment)\n",
    "\n",
    "trainLostList_sgd1_scratch, trainAccList_sgd1_scratch, \\\n",
    "valLossList_sgd1_scratch, valAccList_sgd1_scratch,_,_  = train_model_manually(modelManual, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                              nesterov_momentum, weight_decay, measure_alignment, validate_model = True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelManualDFA = MLPManual(input_dim, learn_rate, loss_type, \"DFA\", B_initialization, optim, device, measure_alignment)\n",
    "trainLostList_sgd1_dfa, trainAccList_sgd1_dfa, \\\n",
    "valLossList_sgd1_dfa, valAccList_sgd1_dfa,_,_  =  train_model_manually(modelManualDFA, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                        nesterov_momentum, weight_decay, measure_alignment, validate_model = True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### For k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "model3 = MLP(k,\"ReLU\", loss_type)\n",
    "optimizer = torch.optim.Adadelta(model3.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
    "\n",
    "trainLostList_Ada3, trainAccList_Ada3, \\\n",
    "valLossList_Ada3, valAccList_Ada3  = train_model(model3, k, trainset, testset, loss_type, loss_fn, optimizer, num_epochs, batch_size, validate_model = True, \n",
    "                                                 performance=accuracy, device=device, lr_scheduler=None)\n",
    "\n",
    "plot_loss_accuracy(trainLostList_Ada3,valLossList_Ada3,trainAccList_Ada3,valAccList_Ada3,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = MLP(k, \"ReLU\", loss_type)\n",
    "optimizer = torch.optim.SGD(model4.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
    "\n",
    "trainLostList_sgd3, trainAccList_sgd3, valLossList_sgd3, valAccList_sgd3  = train_model(model4, k, trainset, testset, loss_type, loss_fn, optimizer, num_epochs, \n",
    "                                                                                        batch_size, validate_model = True, performance=accuracy, device=device, \n",
    "                                                                                        lr_scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "input_dim = 28 * 28 * k \n",
    "modelManual3 = MLPManual(input_dim, learn_rate, loss_type, \"BP\", None, optim, device, measure_alignment)\n",
    "trainLostList_sgd3_scratch, trainAccList_sgd3_scratch, \\\n",
    "valLossList_sgd3_scratch, valAccList_sgd3_scratch,_,_  = train_model_manually(modelManual3, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                              nesterov_momentum, weight_decay, measure_alignment, validate_model = True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.02 # one of the best lr that I got for uniform B, with 0.05 training didn't perform well\n",
    "modelManual3DFA = MLPManual(input_dim, learn_rate, loss_type, \"DFA\", B_initialization, optim, device, measure_alignment)\n",
    "trainLostList_sgd3_dfa, trainAccList_sgd3_dfa, \\\n",
    "valLossList_sgd3_dfa, valAccList_sgd3_dfa,_,_  = train_model_manually(modelManual3DFA, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                      nesterov_momentum, weight_decay, measure_alignment, validate_model = True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax3) = plt.subplots(1, 2, figsize=(16,8))\n",
    "\n",
    "ax1.plot(range(1,21),valAccList_sgd1, color = \"blue\", label = \"SGD BP Pytorch\") \n",
    "ax1.plot(range(1,21),valAccList_Ada1, color = \"green\", label = \"Adadelta BP Pytorch\")\n",
    "ax1.plot(range(1,21),valAccList_sgd1_scratch, color = \"orange\", label = \"SGD BP Dogan\") \n",
    "ax1.plot(range(1,21),valAccList_sgd1_dfa, color = \"red\", label = \"SGD DFA Dogan\")\n",
    "ax1.set_ylim(0.40,1.05)\n",
    "ax1.set_title(\"Test Accuracy k=1\")\n",
    "ax1.set_xlabel(\"Iteration\")\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax1.set_xticks(range(1,21))\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "\n",
    "ax3.plot(range(1,21),valAccList_sgd3, color = \"blue\", label = \"SGD BP Pytorch\")\n",
    "ax3.plot(range(1,21),valAccList_Ada3, color = \"green\", label = \"Adadelta BP Pytorch\")\n",
    "ax3.plot(range(1,21),valAccList_sgd3_scratch, color = \"orange\", label = \"SGD BP Dogan\")\n",
    "ax3.plot(range(1,21),valAccList_sgd3_dfa, color = \"red\", label = \"SGD DFA Dogan\")\n",
    "ax3.set_ylim(0.40,1.05)\n",
    "ax3.set_title(\"Test Accuracy k=3\")\n",
    "ax3.set_xlabel(\"Iteration\")\n",
    "ax3.set_ylabel(\"\")\n",
    "ax3.set_xticks(range(1,21))\n",
    "ax3.grid(True)\n",
    "\n",
    "fig.savefig(\"plots/k13_SGD_Ada_BP_DFA.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with the same weights (SGD BP Pytorch vs SGD BP Dogan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "modelx = MLP(k, \"ReLU\", loss_type).to(device)\n",
    "\n",
    "w1 = copy.deepcopy(modelx.state_dict()[\"layer1.weight\"]).to(device)\n",
    "w2 = copy.deepcopy(modelx.state_dict()[\"layer2.weight\"]).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(modelx.parameters(), lr=learn_rate)\n",
    "\n",
    "trainLostList_sgd3_w, trainAccList_sgd3_w, valLossList_sgd3_w, valAccList_sgd3_w = train_model(modelx, k, trainset, testset, loss_type, loss_fn, optimizer, num_epochs, \n",
    "                                                                                                batch_size, validate_model = True, performance=accuracy, device=device, \n",
    "                                                                                                lr=learn_rate, lr_scheduler=None, updateWManually=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "modelManualx = MLPManual(input_dim, learn_rate, loss_type, \"BP\", None, optim, device, measure_alignment, True, (w1.t(),w2.t()))\n",
    "trainLostList_sgd3_scratch_w, trainAccList_sgd3_scratch_w, \\\n",
    "valLossList_sgd3_scratch_w, valAccList_sgd3_scratch_w, _,_  = train_model_manually(modelManualx, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                                   nesterov_momentum, weight_decay, measure_alignment, validate_model = True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(range(1,21),valAccList_sgd3_w, color = \"blue\", label = \"BP SGD Pytorch\")\n",
    "plt.plot(range(1,21),valAccList_sgd3_scratch_w, color = \"green\", label = \"BP SGD Dogan\")\n",
    "\n",
    "plt.ylim(0.4,1.05)\n",
    "plt.title(\"Test Accuracy k=3\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig(\"plots/k3_SGD_BP_sameWeights.png\")\n",
    "\n",
    "plt.show();\n",
    "\n",
    "# They are gonna be different, because I recreate the data every epoch\n",
    "# but the pattern should be very similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFA Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run DFA up to 50-100 epochs to see if we get similar result as BP\n",
    "k=3\n",
    "input_dim = 28 * 28 * k \n",
    "learn_rate = 0.013\n",
    "measure_alignment_DFA = True\n",
    "modelManual4 = MLPManual(input_dim, learn_rate, loss_type, \"DFA\", B_initialization, optim, device, measure_alignment_DFA)\n",
    "trainLostList_sgad4_scratch, trainAccList_sgd4_scratch, \\\n",
    "valLossList_sgd4_scratch, valAccList_sgd4_scratch, \\\n",
    "similarity_w2B, similarity_w1_grads  = train_model_manually(modelManual4, k, trainset, testset, loss_type, loss_fn, 100, batch_size, momentum,\n",
    "                                                            nesterov_momentum, weight_decay, measure_alignment_DFA, validate_model = True, device=device)\n",
    "\n",
    "plt.figure(figsize=(16,8))                                                                          \n",
    "plotValAccuracy(valAccList_sgd4_scratch, 100, \"DFA Validation\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #tune SGD for 50 epochs before alignement\n",
    "# manual_dfa_sgd_lr_array = np.linspace(0.016, 0.018, 6)\n",
    "# best_manual_dfa_sgd = tuneLearningRate_Manual(manual_dfa_sgd_lr_array, \"DFA\", \"uniform\", \"SGD\", k, trainset, testset, loss_type, loss_fn, device, 50, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the results for DFA and BP by running three times\n",
    "k = 3\n",
    "input_dim = 28 * 28 * k \n",
    "num_epochs = 50\n",
    "measure_alignment = True\n",
    "loss_type = \"Binary Cross Entropy\"\n",
    "# you may need to reduce the learning rate if you observe overfitting, this value is from manual tuning\n",
    "learn_rate = 0.0165\n",
    "\n",
    "df_gen = pd.DataFrame(columns=[\"epoch\", \"DFA SGD\", \"Alignment\", \"Alignment_w1\"])\n",
    "\n",
    "for i in range(1,4):    \n",
    "\n",
    "    modelManualDFA = MLPManual(input_dim, learn_rate, loss_type, \"DFA\", \"uniform\", \"SGD\", device, measure_alignment)\n",
    "    trainLostList_dfa, trainAccList_dfa, valLossList_dfa, valAccList_dfa , \\\n",
    "    similarity_w2B, similarity_w1_grads  =                 train_model_manually(modelManualDFA, k, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                                batch_size, momentum, nesterov_momentum, weight_decay, measure_alignment,\n",
    "                                                                                validate_model = True, device=device)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df[\"epoch\"] = list(range(1,51))\n",
    "    df[\"DFA SGD\"] = valAccList_dfa\n",
    "    df[\"Alignment\"] = similarity_w2B\n",
    "    df[\"Alignment_w1\"] = similarity_w1_grads\n",
    "\n",
    "    df_gen = df_gen.append(df, ignore_index=True)\n",
    "\n",
    "df_gen.to_csv(\"runs/K3_SGD_DFA50epochs.csv\", index=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen = pd.read_csv(\"runs/K3_SGD_DFA50epochs.csv\")\n",
    "\n",
    "fig, (ax1,  ax3) = plt.subplots(1, 2, figsize=(16,8))\n",
    "\n",
    "ticks = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "tickLabels = map(str, ticks)\n",
    "\n",
    "ticks2 = [-0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "tickLabels2 = map(str, ticks2)\n",
    "\n",
    "ax1 = sns.lineplot(data=df_gen, x=\"epoch\", y=\"DFA SGD\", ax=ax1)\n",
    "ax1.set_ylim(0.0,1.0)\n",
    "ax1.set_title(\"DFA k=3\")\n",
    "ax1.set_ylabel(\"Test \\nAccuracy\", rotation=0, labelpad=30)\n",
    "ax1.yaxis.set_label_coords(-0.25,0.5)\n",
    "ax1.set_xlabel(\"\")\n",
    "ax1.set_xticks(range(1,51))\n",
    "ax1.set_yticks(ticks)\n",
    "ax1.set_yticklabels(tickLabels)\n",
    "ax1.grid(True)\n",
    "\n",
    "ax3 = sns.lineplot(data=df_gen, x=\"epoch\", y=\"Alignment\", color=\"purple\", ax=ax3)\n",
    "ax3.set_ylim(-0.1,1.0)\n",
    "ax3.set_title(\"Alignment of $B$ and $w^T$\")\n",
    "h2 = ax3.set_ylabel(\"Cosine \\nSimilarity\", rotation=0, labelpad=30)\n",
    "h2.set_rotation(0)\n",
    "ax3.yaxis.set_label_coords(-0.25,0.5)\n",
    "ax3.set_xlabel(\"\")\n",
    "ax3.set_xticks(range(1,51))\n",
    "ax3.set_yticks(ticks2)\n",
    "ax3.set_yticklabels(tickLabels2)\n",
    "ax3.tick_params()\n",
    "ax3.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.setp(ax1.get_xticklabels(), rotation=90, horizontalalignment='left', fontsize=12)\n",
    "plt.setp(ax3.get_xticklabels(), rotation=90, horizontalalignment='left', fontsize=12)\n",
    "fig.supxlabel('Epoch', x = 0.54, y = -0.05)\n",
    "fig.savefig(\"plots/k3_SGD_DFA_Alingment.png\")\n",
    "fig.savefig(\"../../2-writing/oxforddown/figures/3_k3_SGD_DFA_Alingment.png\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #tune rmsprop for 50 epochs before alignement\n",
    "# k=3\n",
    "# manual_dfa_rmsprop_lr_array = np.linspace(0.00005, 0.0001, 6)\n",
    "# best_manual_dfa_rmsprop = tuneLearningRate_Manual(manual_dfa_rmsprop_lr_array, \"DFA\", \"uniform\", \"RMSProp\", k, trainset, testset, loss_type, loss_fn, device, 50, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the results for DFA and BP by running three times\n",
    "k = 3\n",
    "num_epochs = 50\n",
    "measure_alignment = True\n",
    "loss_type = \"Binary Cross Entropy\"\n",
    "input_dim = 28 * 28 * k \n",
    "learn_rate = 9e-05\n",
    "\n",
    "df_gen_rms = pd.DataFrame(columns=[\"epoch\", \"DFA SGD\", \"Alignment\", \"Alignment_w1\"])\n",
    "\n",
    "for i in range(1,4):    \n",
    "\n",
    "    modelManualDFA = MLPManual(input_dim, learn_rate, loss_type, \"DFA\", \"uniform\", \"RMSProp\", device, measure_alignment)\n",
    "    trainLostList_dfa, trainAccList_dfa, valLossList_dfa, valAccList_dfa , \\\n",
    "    similarity_w2B, similarity_w1_grads  =                 train_model_manually(modelManualDFA, k, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                                batch_size, momentum, nesterov_momentum, weight_decay, measure_alignment,\n",
    "                                                                                validate_model = True, device=device)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df[\"epoch\"] = list(range(1,51))\n",
    "    df[\"DFA SGD\"] = valAccList_dfa\n",
    "    df[\"Alignment\"] = similarity_w2B\n",
    "    df[\"Alignment_w1\"] = similarity_w1_grads\n",
    "\n",
    "    df_gen_rms = df_gen_rms.append(df, ignore_index=True)\n",
    "\n",
    "df_gen_rms.to_csv(\"runs/K3_rms_DFA50epochs.csv\", index=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen = pd.read_csv(\"runs/K3_SGD_DFA50epochs.csv\")\n",
    "df_gen_ada = pd.read_csv(\"runs/K3_rms_DFA50epochs.csv\")\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(16,8), sharex=True, sharey=True)\n",
    "\n",
    "ticks = [-0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "tickLabels = map(str, ticks)\n",
    "ticks2 = [-0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "tickLabels2 = map(str, ticks)\n",
    "\n",
    "ax[0 , 0] = sns.lineplot(data=df_gen, x=\"epoch\", y=\"Alignment\", color=\"purple\", ax=ax[0 , 0])\n",
    "ax[0 , 0].set_ylim(-0.1,1.0)\n",
    "ax[0 , 0].set_title(\"Alignment of $B$ and $w^T$ SGD\")\n",
    "h2 = ax[0 , 0].set_ylabel(\"\", rotation=0, fontsize=12, labelpad=30)\n",
    "h2.set_rotation(0)\n",
    "ax[0 , 0].set_xlabel(\"\")\n",
    "ax[0 , 0].set_xticks(range(1,51))\n",
    "ax[0 , 0].set_yticks(ticks)\n",
    "ax[0 , 0].set_yticklabels(tickLabels)\n",
    "ax[0 , 0].tick_params()\n",
    "ax[0 , 0].grid(True)\n",
    "\n",
    "ax[0 , 1] = sns.lineplot(data=df_gen_ada, x=\"epoch\", y=\"Alignment\", color=\"purple\", ax=ax[0 , 1])\n",
    "ax[0 , 1].set_ylim(-0.1,1.0)\n",
    "ax[0 , 1].set_title(\"Alignment of $B$ and $w^T$ RMSProp\")\n",
    "h2 = ax[0 , 1].set_ylabel(\"\", rotation=0, labelpad=30)\n",
    "h2.set_rotation(0)\n",
    "ax[0 , 1].set_xlabel(\"\")\n",
    "ax[0 , 1].set_xticks(range(1,51))\n",
    "ax[0 , 1].tick_params()\n",
    "ax[0 , 1].grid(True)\n",
    "\n",
    "ax[1 , 0] = sns.lineplot(data=df_gen, x=\"epoch\", y=\"Alignment_w1\", ax=ax[1 , 0])\n",
    "ax[1 , 0].set_ylim(0.0,1.0)\n",
    "ax[1 , 0].set_title(\"Alignment of $w1$ Gradients SGD\")\n",
    "ax[1 , 0].set_ylabel(\"\", rotation=0, labelpad=30)\n",
    "ax[1 , 0].set_xlabel(\"\")\n",
    "ax[1 , 0].set_xticks(range(1,51))\n",
    "ax[1 , 0].set_yticks(ticks2)\n",
    "ax[1 , 0].set_yticklabels(tickLabels2)\n",
    "ax[1 , 0].grid(True)\n",
    "\n",
    "ax[1 , 1] = sns.lineplot(data=df_gen_ada, x=\"epoch\", y=\"Alignment_w1\", ax=ax[1 , 1])\n",
    "ax[1 , 1].set_ylim(0.0,1.0)\n",
    "ax[1 , 1].set_title(\"Alignment of $w1$ Gradients RMSProp\")\n",
    "ax[1 , 1].set_ylabel(\"\", rotation=0, labelpad=30)\n",
    "ax[1 , 1].set_xlabel(\"\")\n",
    "ax[1 , 1].set_xticks(range(1,51))\n",
    "ax[1 , 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.setp(ax[0 , 0].get_xticklabels(), rotation=90, horizontalalignment='left', fontsize=12)\n",
    "plt.setp(ax[0 , 1].get_xticklabels(), rotation=90, horizontalalignment='left', fontsize=12)\n",
    "plt.setp(ax[1 , 0].get_xticklabels(), rotation=90, horizontalalignment='left', fontsize=12)\n",
    "plt.setp(ax[1 , 1].get_xticklabels(), rotation=90, horizontalalignment='left', fontsize=12)\n",
    "fig.supxlabel('Epoch', x = 0.5, y = -0.05)\n",
    "fig.supylabel('Cosine \\nSimilarity', x = -0.08, y = 0.5 ,rotation=0)\n",
    "fig.savefig(\"plots/k3_SGD_RMSProp_DFA_Alingment.png\", bbox_inches='tight')\n",
    "fig.savefig(\"../../2-writing/oxforddown/figures/3_k3_SGD_RMSProp_DFA_Alingment.png\", bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Matrix Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tune their learning rates to get best one, it is done by checking the last 10 val Accuracy\n",
    "# k=3\n",
    "# lr_array_uni = np.linspace(0.01, 0.025, 6)\n",
    "# lr_array_std_uni = np.linspace(0.0015, 0.0035, 6)\n",
    "# lr_array_gauss = np.linspace(0.01, 0.02, 6)\n",
    "# lr_array_std_gauss = np.linspace(0.0008, 0.0015, 6)\n",
    "\n",
    "# best_manual_dfa_sgd_uni = tuneLearningRate_Manual(lr_array_uni, \"DFA\", \"uniform\", optim, k, trainset, testset, loss_type, loss_fn, device, num_epochs, weight_decay)\n",
    "# best_manual_dfa_sgd_std_uni = tuneLearningRate_Manual(lr_array_std_uni, \"DFA\", \"standard uniform\", optim, k, trainset, testset, loss_type, loss_fn, device, num_epochs, weight_decay)\n",
    "# best_manual_dfa_sgd_gaussian = tuneLearningRate_Manual(lr_array_gauss, \"DFA\", \"gaussian\", optim, k, trainset, testset, loss_type, loss_fn, device, num_epochs, weight_decay)\n",
    "# best_manual_dfa_sgd_std_gaussian = tuneLearningRate_Manual(lr_array_std_gauss, \"DFA\", \"standard gaussian\", optim, k, trainset, testset, loss_type, loss_fn, device, num_epochs, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning these parameters take too much time, so let's run once and store them statically\n",
    "best_manual_dfa_sgd_uni = 0.02\n",
    "best_manual_dfa_sgd_std_uni = 0.0035\n",
    "best_manual_dfa_sgd_gaussian = 0.015\n",
    "best_manual_dfa_sgd_std_gaussian = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have plot for different learning rates (different lr_array for each random matrix)\n",
    "num_epochs = 20\n",
    "K = 3\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(16,8))\n",
    "lr_array_uni = np.linspace(0.015, 0.025, 3)\n",
    "lr_array_std_uni = np.linspace(0.002, 0.004, 3)\n",
    "lr_array_gauss = np.linspace(0.014, 0.018, 3)\n",
    "lr_array_std_gauss = np.linspace(0.0005, 0.0015, 3)\n",
    "\n",
    "for init in [\"standard uniform\", \"uniform\", \"standard gaussian\", \"gaussian\"]:\n",
    "    if init == \"standard uniform\":\n",
    "        for lr in lr_array_std_uni:\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            modelManualx = MLPManual(input_dim, lr, loss_type, \"DFA\", init, optim, device, measure_alignment)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList ,_,_  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay, measure_alignment,\n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax1, init)\n",
    "    elif init == \"uniform\":\n",
    "        for lr in lr_array_uni:\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            modelManualx = MLPManual(input_dim, lr, loss_type, \"DFA\", init, optim, device, measure_alignment)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList ,_,_  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay, measure_alignment,\n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax2, init)\n",
    "    elif init == \"standard gaussian\":\n",
    "        for lr in lr_array_std_gauss:\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            modelManualx = MLPManual(input_dim, lr, loss_type, \"DFA\", init, optim, device, measure_alignment)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList ,_,_  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay, measure_alignment,\n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax3, init)\n",
    "    elif init == \"gaussian\":\n",
    "        for lr in lr_array_gauss:\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            modelManualx = MLPManual(input_dim, lr, loss_type, \"DFA\", init, optim, device, measure_alignment)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList ,_,_  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay, measure_alignment,\n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax4, init)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"plots/k3_SGD_DFA_BInitsWDifferentlrs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the results for DFA and BP by running three times\n",
    "k = 3\n",
    "input_dim = k * 784\n",
    "num_epochs = 20\n",
    "measure_alignment = False\n",
    "loss_type = \"Binary Cross Entropy\"\n",
    "\n",
    "df_gen = pd.DataFrame(columns=[\"epoch\", \"standard uniform\", \"uniform\", \"standard gaussian\", \"gaussian\"])\n",
    "\n",
    "for i in range(1,4):    \n",
    "\n",
    "    modelManualDFA_uni = MLPManual(input_dim, best_manual_dfa_sgd_uni, loss_type, \"DFA\", \"uniform\", \"SGD\", device, measure_alignment)\n",
    "    modelManualDFA_std_uni = MLPManual(input_dim, best_manual_dfa_sgd_std_uni, loss_type, \"DFA\", \"standard uniform\", \"SGD\", device, measure_alignment)\n",
    "    modelManualDFA_gauss = MLPManual(input_dim, best_manual_dfa_sgd_gaussian, loss_type, \"DFA\", \"gaussian\", \"SGD\", device, measure_alignment)\n",
    "    modelManualDFA_std_gauss = MLPManual(input_dim, best_manual_dfa_sgd_std_gaussian, loss_type, \"DFA\", \"standard gaussian\", \"SGD\", device, measure_alignment)\n",
    "\n",
    "\n",
    "    _, _, _, valAccList_dfa_uni , _, _  = train_model_manually(modelManualDFA_uni, k, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                  batch_size, momentum, nesterov_momentum, weight_decay, measure_alignment,\n",
    "                                                                  validate_model = True, device=device)\n",
    "\n",
    "    _, _, _, valAccList_dfa_std_uni , _, _  = train_model_manually(modelManualDFA_std_uni, k, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                  batch_size, momentum, nesterov_momentum, weight_decay, measure_alignment,\n",
    "                                                                  validate_model = True, device=device)\n",
    "\n",
    "    _, _, _, valAccList_dfa_gauss , _, _  = train_model_manually(modelManualDFA_gauss, k, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                  batch_size, momentum, nesterov_momentum, weight_decay, measure_alignment,\n",
    "                                                                  validate_model = True, device=device)    \n",
    "\n",
    "    _, _, _, valAccList_dfa_std_gauss , _, _  = train_model_manually(modelManualDFA_std_gauss, k, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                  batch_size, momentum, nesterov_momentum, weight_decay, measure_alignment,\n",
    "                                                                  validate_model = True, device=device)                                                     \n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df[\"epoch\"] = list(range(1,21))\n",
    "    df[\"uniform\"] = valAccList_dfa_uni\n",
    "    df[\"standard uniform\"] = valAccList_dfa_std_uni\n",
    "    df[\"gaussian\"] = valAccList_dfa_gauss\n",
    "    df[\"standard gaussian\"] = valAccList_dfa_std_gauss\n",
    "\n",
    "    df_gen = df_gen.append(df, ignore_index=True)\n",
    "\n",
    "df_genv2 = df_gen.melt(id_vars=[\"epoch\"], var_name=\"randommatrix\", value_name=\"testaccuracy\")\n",
    "df_gen.to_csv(\"runs/K3_SGD_DFA_RandomMatrices.csv\", index=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen = pd.read_csv(\"runs/K3_SGD_DFA_RandomMatrices.csv\")\n",
    "g = sns.lineplot(data=df_gen, x=\"epoch\", y=\"uniform\", alpha=.6, label = \"uniform\")\n",
    "g = sns.lineplot(data=df_gen, x=\"epoch\", y=\"standard uniform\", alpha=.6, label=\"standard uniform\")\n",
    "g = sns.lineplot(data=df_gen, x=\"epoch\", y=\"gaussian\", alpha=.6, label=\"gaussian\")\n",
    "g = sns.lineplot(data=df_gen, x=\"epoch\", y=\"standard gaussian\", alpha=.6, label=\"standard gaussian\")\n",
    "\n",
    "plt.grid()\n",
    "plt.ylabel(\"Test \\nAccuracy\", rotation=0, labelpad=30)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylim(0.4,1)\n",
    "plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "modelManualDFA_uni = MLPManual(input_dim, best_manual_dfa_sgd_uni, loss_type, \"DFA\", \"uniform\", \"SGD\", device, measure_alignment)\n",
    "modelManualDFA_std_uni = MLPManual(input_dim, best_manual_dfa_sgd_std_uni, loss_type, \"DFA\", \"standard uniform\", \"SGD\", device, measure_alignment)\n",
    "modelManualDFA_gauss = MLPManual(input_dim, best_manual_dfa_sgd_gaussian, loss_type, \"DFA\", \"gaussian\", \"SGD\", device, measure_alignment)\n",
    "modelManualDFA_std_gauss = MLPManual(input_dim, best_manual_dfa_sgd_std_gaussian, loss_type, \"DFA\", \"standard gaussian\", \"SGD\", device, measure_alignment)\n",
    "\n",
    "df = pd.DataFrame(columns = [\"Random Matrix\", \"Optimizer\", \"Results\"])\n",
    "idx = 0\n",
    "for model in [modelManualDFA_uni, modelManualDFA_std_uni, modelManualDFA_gauss, modelManualDFA_std_gauss]:\n",
    "    liste = getMeanStd(model,k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, \n",
    "                       momentum, nesterov_momentum, weight_decay, measure_alignment, True, device)\n",
    "    random_Matrix = model.B_initialization\n",
    "    optim = model.optim\n",
    "    for value in liste:\n",
    "        df.loc[idx,:] = [random_Matrix,optim,value]\n",
    "        idx += 1\n",
    "\n",
    "df[\"Error\"] = df[\"Results\"].apply(lambda x: 1-x)\n",
    "df.to_csv(\"runs/k3_DFA_RandomMatrices.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"runs/k3_DFA_RandomMatrices.csv\")\n",
    "\n",
    "ticks = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "tickLabels = map(str, ticks)\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=df, kind=\"bar\",\n",
    "    x=\"Random Matrix\", y=\"Results\", alpha=.6)\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"\", \"Test \\nAccuracy\", rotation=0, labelpad=30)\n",
    "g.ax.yaxis.set_label_coords(-0.15,0.5)\n",
    "g.fig.suptitle(\"DFA with Various Random Matrices\")\n",
    "g.fig.set_size_inches(12,8)\n",
    "plt.grid()\n",
    "plt.ylim(0,1)\n",
    "plt.yticks([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "g.tight_layout();\n",
    "g.savefig(\"plots/k3_DFA_RandomMatrices.png\")\n",
    "g.savefig(\"../../2-writing/oxforddown/figures/3_k3_DFA_RandomMatrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tune their learning rates to get best one, it is done by checking the last 10 val Accuracy\n",
    "# k=3\n",
    "\n",
    "# lr_array_adagrad = np.linspace(0.003, 0.006, 6)\n",
    "# lr_array_adadelta = np.linspace(0.1, 1, 6)\n",
    "# lr_array_rmsprop = np.linspace(0.0003, 0.0006, 6)\n",
    "# lr_array_adam = np.linspace(0.001, 0.002, 6)\n",
    "\n",
    "# best_manual_dfa_adagrad_uni = tuneLearningRate_Manual(lr_array_adagrad, \"DFA\", B_initialization, \"Adagrad\", k, trainset, testset, loss_type,loss_fn, device, num_epochs, weight_decay)\n",
    "# best_manual_dfa_adadelta_uni = tuneLearningRate_Manual(lr_array_adadelta, \"DFA\", B_initialization, \"Adadelta\", k, trainset, testset, loss_type, loss_fn, device, num_epochs, weight_decay)    \n",
    "# best_manual_dfa_rmsprop_uni = tuneLearningRate_Manual(lr_array_rmsprop, \"DFA\", B_initialization, \"RMSProp\", k, trainset, testset, loss_type, loss_fn, device, num_epochs, weight_decay)\n",
    "# best_manual_dfa_adam_uni = tuneLearningRate_Manual(lr_array_adam, \"DFA\", B_initialization, \"Adam\", k, trainset, testset, loss_type, loss_fn, device, num_epochs, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_manual_dfa_adagrad_uni = 0.0036\n",
    "best_manual_dfa_adadelta_uni = 1.0\n",
    "best_manual_dfa_rmsprop_uni = 0.00024\n",
    "best_manual_dfa_adam_uni = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have plot for different learning rates (different lr_array for each random matrix)\n",
    "num_epochs = 20\n",
    "K = 3\n",
    "fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(16,8))\n",
    "lr_array_sgd = np.linspace(0.015, 0.035, 3)\n",
    "lr_array_adagrad = np.linspace(0.001, 0.005, 3)\n",
    "lr_array_adadelta = np.linspace(0.8, 1, 3)\n",
    "lr_array_rmsprop = np.linspace(0.0001, 0.0005, 3)\n",
    "lr_array_adam = np.linspace(0.001, 0.004, 3)\n",
    "\n",
    "for optim in [\"SGD\", \"Adagrad\", \"Adadelta\", \"RMSProp\", \"Adam\"]:\n",
    "    if optim == \"SGD\":\n",
    "        for lr in lr_array_sgd:\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            modelManualx = MLPManual(input_dim, lr, loss_type, \"DFA\", B_initialization, optim, device, measure_alignment)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList ,_,_  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay, measure_alignment,\n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax1, optim)\n",
    "\n",
    "    elif optim == \"Adagrad\":\n",
    "        for lr in lr_array_adagrad:\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            modelManualx = MLPManual(input_dim, lr, loss_type, \"DFA\", B_initialization, optim, device, measure_alignment)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList ,_,_  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay, measure_alignment,\n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax2, optim)\n",
    "    elif optim == \"Adadelta\":\n",
    "        for lr in lr_array_adadelta:\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            modelManualx = MLPManual(input_dim, lr, loss_type, \"DFA\", B_initialization, optim, device, measure_alignment)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList ,_,_  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay, measure_alignment,\n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax3, optim)\n",
    "    elif optim == \"RMSProp\":\n",
    "        for lr in lr_array_rmsprop:\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            modelManualx = MLPManual(input_dim, lr, loss_type, \"DFA\", B_initialization, optim, device, measure_alignment)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList ,_,_  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay, measure_alignment,\n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax4, optim)\n",
    "    else:\n",
    "        for lr in lr_array_adam:\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            modelManualx = MLPManual(input_dim, lr, loss_type, \"DFA\", B_initialization, optim, device, measure_alignment)\n",
    "            trainLostList, trainAccList, \\\n",
    "            valLossList, valAccList ,_,_  = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay, measure_alignment,\n",
    "                                                            validate_model = True, device=device)\n",
    "            fillSubplot(valAccList, num_epochs, str(round(lr,4)), ax5, optim)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(\"plots/k3_All_DFA_optimsWDifferentlrs.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a plot with different B initialization methods with 20 epochs with their best lr\n",
    "optims = [\"SGD\", \"Adagrad\", \"Adadelta\", \"RMSProp\", \"Adam\"]\n",
    "num_epochs=20\n",
    "K=3\n",
    "\n",
    "fig = plt.figure(figsize=(15,9))\n",
    "for optim in optims:\n",
    "    if optim == \"SGD\":\n",
    "        modelManualx = MLPManual(input_dim, best_manual_dfa_sgd_uni, loss_type, \"DFA\", B_initialization, optim, device, measure_alignment)\n",
    "    elif optim == \"Adagrad\":\n",
    "        modelManualx = MLPManual(input_dim, best_manual_dfa_adagrad_uni, loss_type, \"DFA\", B_initialization, optim, device, measure_alignment)\n",
    "    elif optim == \"Adadelta\":\n",
    "        modelManualx = MLPManual(input_dim, best_manual_dfa_adadelta_uni, loss_type, \"DFA\", B_initialization, optim, device, measure_alignment)\n",
    "    elif optim == \"RMSProp\":\n",
    "        modelManualx = MLPManual(input_dim, best_manual_dfa_rmsprop_uni, loss_type, \"DFA\", B_initialization, optim, device, measure_alignment)\n",
    "    elif optim == \"Adam\":\n",
    "        modelManualx = MLPManual(input_dim, best_manual_dfa_adam_uni, loss_type, \"DFA\", B_initialization, optim, device, measure_alignment)\n",
    "\n",
    "    trainLostList, trainAccList, \\\n",
    "    valLossList, valAccList ,_,_ = train_model_manually(modelManualx, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                            batch_size, momentum, nesterov_momentum, weight_decay, measure_alignment,\n",
    "                                                            validate_model = True, device=device)\n",
    "    plotValAccuracy(valAccList, num_epochs, optim, K)\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.savefig(\"plots/k3_All_DFA_bestoptims.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 3\n",
    "\n",
    "# torch_bp_sgd_lr_array_ = np.linspace(0.05, 0.15, 6)\n",
    "# torch_bp_ada_lr_array = np.linspace(0.10, 0.20, 6)\n",
    "\n",
    "# best_torch_bp_sgd = tuneLearningRate_Torch(torch_bp_sgd_lr_array_, \"SGD\", k, trainset, testset, loss_type, loss_fn, device, batch_size, num_epochs, weight_decay)\n",
    "# best_torch_bp_ada = tuneLearningRate_Torch(torch_bp_ada_lr_array, \"Adadelta\", k, trainset, testset, loss_type, loss_fn, device, batch_size, num_epochs, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_torch_bp_sgd = 0.13\n",
    "best_torch_bp_ada = 0.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce the experiments from Learning Parities with Neural Networks\n",
    "learn_rate = 0.05\n",
    "num_epochs = 20\n",
    "loss_type = \"Binary Cross Entropy\"\n",
    "fig, (ax1, ax3) = plt.subplots(1, 2, figsize=(16,8))\n",
    "for K in [1,3]:\n",
    "    for activation in [\"Adadelta\", \"NTK\", \"Gaussian features\", \"ReLU features\", \"SGD\"]:\n",
    "        model = MLP(K, activation, loss_type)\n",
    "        if \"features\" in activation:\n",
    "            # deactivate the first layer\n",
    "            optimizer = torch.optim.Adadelta(model.layer2.parameters(), lr = learn_rate, weight_decay=weight_decay)\n",
    "        elif \"NTK\" in activation:\n",
    "            paramsToUpdate = list(model.layer1.parameters()) + list(model.layer2.parameters())\n",
    "            optimizer = torch.optim.Adadelta(paramsToUpdate, lr = learn_rate, weight_decay=weight_decay)\n",
    "        elif \"SGD\" in activation:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr = best_torch_bp_sgd, weight_decay=weight_decay)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adadelta(model.parameters(), lr = best_torch_bp_ada, weight_decay=weight_decay)\n",
    "\n",
    "        print(\"Activation:\",activation)\n",
    "\n",
    "        trainLostList, trainAccList, valLossList, valAccList  = train_model(model, K, trainset, testset, loss_type, loss_fn, optimizer, num_epochs,\n",
    "                                                                            batch_size, validate_model = True, performance=accuracy,\n",
    "                                                                            device=device, lr_scheduler=None)\n",
    "\n",
    "        if K == 1:\n",
    "            fillSubplot(valAccList, num_epochs, activation, ax1, \"k = \" + str(K))\n",
    "        else:\n",
    "            fillSubplot(valAccList, num_epochs, activation, ax3, \"k = \" + str(K))\n",
    "\n",
    "ax3.legend().set_visible(False)\n",
    "fig.tight_layout()\n",
    "fig.supxlabel('Epoch', x = 0.51, y = -0.05)\n",
    "fig.supylabel('Test \\nAccuracy', rotation=0, x = -0.075)\n",
    "fig.savefig(\"plots/k13_SGD_ada_BP_reproduced.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual_bp_sgd_lr_array = np.linspace(0.01, 0.07, 6)\n",
    "# manual_bp_adagrad_lr_array = np.linspace(0.005, 0.01, 6)\n",
    "# manual_bp_adadelta_lr_array = np.linspace(0.1, 1, 6)\n",
    "# manual_bp_rmsprop_lr_array = np.linspace(0.0001, 0.0005, 6)\n",
    "# manual_bp_adam_lr_array = np.linspace(0.001, 0.005, 6)\n",
    "\n",
    "\n",
    "# best_manual_bp_sgd = tuneLearningRate_Manual(manual_bp_sgd_lr_array, \"BP\", None, \"SGD\", k, trainset, testset, loss_type, loss_fn, device, num_epochs, weight_decay)\n",
    "# best_manual_bp_adagrad = tuneLearningRate_Manual(manual_bp_adagrad_lr_array, \"BP\", None, \"Adagrad\", k, trainset, testset, loss_type, loss_fn, device, num_epochs, weight_decay)\n",
    "# best_manual_bp_adadelta = tuneLearningRate_Manual(manual_bp_adadelta_lr_array, \"BP\", None, \"Adadelta\", k, trainset, testset, loss_type, loss_fn, device, num_epochs, weight_decay)\n",
    "# best_manual_bp_rmsprop = tuneLearningRate_Manual(manual_bp_rmsprop_lr_array, \"BP\", None, \"RMSProp\", k, trainset, testset, loss_type, loss_fn, device, num_epochs, weight_decay)\n",
    "# best_manual_bp_adam = tuneLearningRate_Manual(manual_bp_adam_lr_array, \"BP\", None, \"Adam\", k, trainset, testset, loss_type, loss_fn, device, num_epochs, weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_manual_bp_sgd = 0.07\n",
    "best_manual_bp_adagrad = 0.008\n",
    "best_manual_bp_adadelta = 1.0\n",
    "best_manual_bp_rmsprop = 0.00026\n",
    "best_manual_bp_adam = 0.0018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the results for DFA and BP by running three times\n",
    "K = 3\n",
    "num_epochs = 20\n",
    "measure_alignment = False\n",
    "loss_type = \"Binary Cross Entropy\"\n",
    "\n",
    "df_gen1 = pd.DataFrame(columns=[\"epoch\", \"BP SGD\", \"DFA SGD\"])\n",
    "df_gen3 = pd.DataFrame(columns=[\"epoch\", \"BP SGD\", \"DFA SGD\"])\n",
    "for K in [1,3]:\n",
    "    for i in range(1,4):\n",
    "        modelManualBP = MLPManual(input_dim, best_manual_bp_sgd, loss_type, \"BP\", None, \"SGD\", device, measure_alignment)\n",
    "        trainLostList_bp, trainAccList_bp, valLossList_bp, valAccList_bp ,_,_  = train_model_manually(modelManualBP, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                                                        batch_size, momentum, nesterov_momentum, weight_decay, measure_alignment,\n",
    "                                                                                                        validate_model = True, device=device)\n",
    "                                                                                                        \n",
    "        modelManualDFA = MLPManual(input_dim, best_manual_dfa_sgd_uni, loss_type, \"DFA\", \"uniform\", \"SGD\", device, measure_alignment)\n",
    "\n",
    "        trainLostList_dfa, trainAccList_dfa, valLossList_dfa, valAccList_dfa ,_,_  = train_model_manually(modelManualDFA, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                                                            batch_size, momentum, nesterov_momentum, weight_decay, measure_alignment,\n",
    "                                                                                                            validate_model = True, device=device)\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        df[\"epoch\"] = list(range(1,21))\n",
    "        df[\"BP SGD\"] = valAccList_bp\n",
    "        df[\"DFA SGD\"] = valAccList_dfa\n",
    "        if K == 1:\n",
    "            df_gen1 = df_gen1.append(df, ignore_index=True)\n",
    "        else:\n",
    "            df_gen3 = df_gen3.append(df, ignore_index=True)\n",
    "\n",
    "df_gen1.to_csv(\"runs/K1_SGD_BPDFA.csv\", index=False)\n",
    "df_gen3.to_csv(\"runs/K3_SGD_BPDFA.csv\", index=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen1 = pd.read_csv(\"runs/K1_SGD_BPDFA.csv\")\n",
    "df_gen3 = pd.read_csv(\"runs/K3_SGD_BPDFA.csv\")\n",
    "\n",
    "fig, (ax1,  ax3) = plt.subplots(1, 2, figsize=(16,8))\n",
    "\n",
    "ax1 = sns.lineplot(data=df_gen1, x=\"epoch\", y=\"BP SGD\", label=\"BP SGD\", ax=ax1)\n",
    "ax1 = sns.lineplot(data=df_gen1, x=\"epoch\", y=\"DFA SGD\", label = \"DFA SGD\", ax=ax1)\n",
    "ax1.set_ylim(0.40,1.05)\n",
    "ax1.set_title(\"k=1\")\n",
    "ax1.set_ylabel(\"\")\n",
    "ax1.set_xlabel(\"\")\n",
    "ax1.set_xticks(range(1,21))\n",
    "ax1.legend(loc = \"lower left\")\n",
    "ax1.grid(True)\n",
    "\n",
    "\n",
    "ax3 = sns.lineplot(data=df_gen3, x=\"epoch\", y=\"BP SGD\", ax=ax3)\n",
    "ax3 = sns.lineplot(data=df_gen3, x=\"epoch\", y=\"DFA SGD\", ax=ax3)\n",
    "ax3.set_ylim(0.40,1.05)\n",
    "ax3.set_title(\"k=3\")\n",
    "ax3.set_ylabel(\"\")\n",
    "ax3.set_xlabel(\"\")\n",
    "ax3.set_xticks(range(1,21))\n",
    "ax3.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.supxlabel('Epoch', x = 0.51, y = -0.05)\n",
    "fig.supylabel('Test \\nAccuracy', rotation=0, x = -0.08)\n",
    "fig.savefig(\"plots/k13_SGD_DFAvsBP.png\", bbox_inches='tight')\n",
    "fig.savefig(\"../../2-writing/oxforddown/figures/3_k13_SGD_DFAvsBP.png\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "K = 3\n",
    "num_epochs = 20\n",
    "loss_type = \"Binary Cross Entropy\"\n",
    "\n",
    "fig = plt.figure(figsize=(15,9))\n",
    "for activation in [\"BP SGD\", \"DFA SGD\", \"Adadelta BP\", \"Adadelta DFA\"]:\n",
    "    #print(\"Activation:\",activation)\n",
    "    if activation == \"BP SGD\":\n",
    "        modelManual3 = MLPManual(input_dim, best_manual_bp_sgd, loss_type, \"BP\", None, \"SGD\", device, measure_alignment)\n",
    "\n",
    "    elif activation == \"DFA SGD\":\n",
    "        modelManual3 = MLPManual(input_dim, best_manual_dfa_sgd_uni, loss_type, \"DFA\", \"uniform\", \"SGD\", device, measure_alignment)\n",
    "\n",
    "    trainLostList, trainAccList, valLossList, valAccList ,_,_  = train_model_manually(modelManual3, K, trainset, testset, loss_type, loss_fn, num_epochs,\n",
    "                                                                                batch_size, momentum, nesterov_momentum, weight_decay, measure_alignment,\n",
    "                                                                                validate_model = True, device=device)\n",
    "\n",
    "\n",
    "    plotValAccuracy(valAccList, num_epochs, activation, K)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"plots/k3_best_SGD_delta_BP_DFA.png\")\n",
    "plt.show()\n",
    "dataset = MNISTParity(trainset, K, 128)\n",
    "dataset.plotRandomData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare BP and DFA with Adaptive methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "model_bp_sgd = MLPManual(input_dim, best_manual_bp_sgd, loss_type, \"BP\", None, \"SGD\", device, measure_alignment)\n",
    "model_bp_adagrad = MLPManual(input_dim, best_manual_bp_adagrad, loss_type, \"BP\", None, \"Adagrad\", device, measure_alignment)\n",
    "model_bp_rmsprop = MLPManual(input_dim, best_manual_bp_rmsprop, loss_type, \"BP\", None, \"RMSProp\", device, measure_alignment)\n",
    "model_bp_adadelta = MLPManual(input_dim, best_manual_bp_adadelta, loss_type, \"BP\", None, \"Adadelta\", device, measure_alignment)\n",
    "model_bp_adam = MLPManual(input_dim, best_manual_bp_adam, loss_type, \"BP\", None, \"Adam\", device, measure_alignment)\n",
    "\n",
    "model_dfa_sgd = MLPManual(input_dim, best_manual_dfa_sgd_uni, loss_type, \"DFA\", B_initialization, \"SGD\", device, measure_alignment)\n",
    "model_dfa_adagrad = MLPManual(input_dim, best_manual_dfa_adagrad_uni, loss_type, \"DFA\", B_initialization, \"Adagrad\", device, measure_alignment)\n",
    "model_dfa_rmsprop = MLPManual(input_dim, best_manual_dfa_rmsprop_uni, loss_type, \"DFA\", B_initialization, \"RMSProp\", device, measure_alignment)\n",
    "model_dfa_adadelta = MLPManual(input_dim, best_manual_dfa_adadelta_uni, loss_type, \"DFA\", B_initialization, \"Adadelta\", device, measure_alignment)\n",
    "model_dfa_adam = MLPManual(input_dim, best_manual_dfa_adam_uni, loss_type, \"DFA\", B_initialization, \"Adam\", device, measure_alignment)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns = [\"Train_Method\", \"Optimizer\", \"Results\"])\n",
    "idx = 0\n",
    "for model in [model_bp_sgd, model_bp_adagrad, model_bp_rmsprop, model_bp_adadelta, model_bp_adam, model_dfa_sgd, model_dfa_adagrad, model_dfa_rmsprop, model_dfa_adadelta, model_dfa_adam]:\n",
    "    liste = getMeanStd(model,k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, \n",
    "                       momentum, nesterov_momentum, weight_decay, measure_alignment, True, device)\n",
    "    train_Method = model.train_method\n",
    "    optim = model.optim\n",
    "    for value in liste:\n",
    "        df.loc[idx,:] = [train_Method,optim,value]\n",
    "        idx += 1\n",
    "\n",
    "df[\"Error\"] = df[\"Results\"].apply(lambda x: 1-x)\n",
    "df.to_csv(\"runs/k3_All.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"runs/k3_All.csv\")\n",
    "\n",
    "g = sns.catplot(\n",
    "    data=df, kind=\"bar\",\n",
    "    x=\"Optimizer\", y=\"Results\", hue=\"Train_Method\", alpha=.6, legend_out=False)\n",
    "g.despine(left=True)\n",
    "g.set_axis_labels(\"\", \"Test \\nAccuracy\", rotation=0, labelpad=30)\n",
    "g.ax.yaxis.set_label_coords(-0.15,0.5)\n",
    "g.fig.suptitle(\"BP vs DFA with Adaptive Methods\")\n",
    "g.legend.set_title(\"\")\n",
    "g.fig.set_size_inches(12,8)\n",
    "plt.grid()\n",
    "plt.ylim(0,1)\n",
    "plt.yticks([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "g.tight_layout()\n",
    "g.savefig(\"plots/mainExperiment.png\")\n",
    "g.savefig(\"../../2-writing/oxforddown/figures/3_mainExperiment.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_alignment = False\n",
    "k=1\n",
    "input_dim = k * 784\n",
    "optim = \"SGD\"\n",
    "learn_rate = best_manual_bp_sgd\n",
    "modelManual3 = MLPManual(input_dim, learn_rate, loss_type, \"BP\", None, optim, device, measure_alignment)\n",
    "trainLostList_sgd3_scratch, trainAccList_sgd3_scratch, \\\n",
    "valLossList_sgd3_scratch, valAccList_sgd3_scratch ,_,_  = train_model_manually(modelManual3, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                                 nesterov_momentum, weight_decay, measure_alignment, validate_model = True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = MNISTParity(trainset, k, batch_size)\n",
    "y_hat, a1, h1 =  modelManual3(trainData.data[:5000].to(device), modelManual3.w1, modelManual3.w2)\n",
    "tsne = TSNE(n_components=2, verbose=1, random_state=123, n_jobs=-1)\n",
    "z = tsne.fit_transform(h1.cpu().detach().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"y\"] = trainData.targets[:5000].numpy()\n",
    "df[\"y_digits\"] = trainData.original_target[:5000].numpy()\n",
    "df[\"comp-1\"] = z[:,0]\n",
    "df[\"comp-2\"] = z[:,1]\n",
    "\n",
    "fig, (ax1, ax3) = plt.subplots(1, 2, figsize=(16,8))\n",
    "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n",
    "                palette=sns.color_palette(\"hls\", 2),\n",
    "                data=df, ax= ax1).set(title=\"Hidden representation k=1, BP\")\n",
    "ax1.axis(\"off\")\n",
    "\n",
    "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y_digits.tolist(),\n",
    "                palette=sns.color_palette(\"hls\", 10),\n",
    "                data=df).set(title=\"Hidden representation k=1 with Digits BP\")\n",
    "\n",
    "ax3.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "\n",
    "modelManual3 = MLPManual(784*k, best_manual_bp_sgd, loss_type, \"BP\", None, optim, device, measure_alignment)\n",
    "trainLostList_sgd3_scratch, trainAccList_sgd3_scratch, \\\n",
    "valLossList_sgd3_scratch, valAccList_sgd3_scratch ,_,_  = train_model_manually(modelManual3, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                               nesterov_momentum, weight_decay, measure_alignment, validate_model = True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "modelManual3_dfa = MLPManual(784*k, best_manual_dfa_sgd_uni, loss_type, \"DFA\", \"uniform\", optim, device, measure_alignment)\n",
    "trainLostList_sgd3_scratch, trainAccList_sgd3_scratch, \\\n",
    "valLossList_sgd3_scratch, valAccList_sgd3_scratch ,_,_  = train_model_manually(modelManual3_dfa, k, trainset, testset, loss_type, loss_fn, num_epochs, batch_size, momentum,\n",
    "                                                                               nesterov_momentum, weight_decay, measure_alignment, validate_model = True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = MNISTParity(trainset, k, batch_size)\n",
    "y_hat, a1, h1 = modelManual3(trainData.data[:7500].to(device), modelManual3.w1, modelManual3.w2)\n",
    "y_hat_dfa, a1_dfa, h1_dfa = modelManual3_dfa(trainData.data[:7500].to(device), modelManual3_dfa.w1, modelManual3_dfa.w2)\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, random_state=123, n_jobs=-1)\n",
    "z = tsne.fit_transform(h1.cpu().detach().numpy())\n",
    "z_dfa =  tsne.fit_transform(h1_dfa.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"y\"] = trainData.targets[:7500].numpy()\n",
    "df[\"comp-1\"] = z[:,0]\n",
    "df[\"comp-2\"] = z[:,1]\n",
    "df[\"comp-1_dfa\"] = z_dfa[:,0]\n",
    "df[\"comp-2_dfa\"] = z_dfa[:,1]\n",
    "\n",
    "\n",
    "fig, (ax1, ax3) = plt.subplots(1, 2, figsize=(16,8))\n",
    "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n",
    "                palette=sns.color_palette(\"hls\", 2),\n",
    "                data=df, ax= ax1).set(title=\"BP\")\n",
    "ax1.axis(\"off\")\n",
    "ax1.legend().set_visible(False)\n",
    "\n",
    "sns.scatterplot(x=\"comp-1_dfa\", y=\"comp-2_dfa\", hue=df.y.tolist(),\n",
    "                palette=sns.color_palette(\"hls\", 2),\n",
    "                data=df, ax= ax3).set(title=\"DFA\")\n",
    "\n",
    "ax3.legend(loc = \"lower left\", bbox_to_anchor=(-0.18,0.0))\n",
    "ax3.axis(\"off\") \n",
    "plt.suptitle(\"Hidden Representations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = modelManual3.w1\n",
    "W1_dfa = modelManual3_dfa.w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w01 = getW1ForImage(0,W1).cpu() # for example this is the weight matrix for left image\n",
    "w11 = getW1ForImage(1,W1).cpu()\n",
    "w21 = getW1ForImage(2,W1).cpu()\n",
    "\n",
    "w01_dfa = getW1ForImage(0,W1_dfa).cpu() # for example this is the weight matrix for left image\n",
    "w11_dfa = getW1ForImage(1,W1_dfa).cpu()\n",
    "w21_dfa = getW1ForImage(2,W1_dfa).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the hidden representations for each image\n",
    "hidList = []\n",
    "hidList_dfa = []\n",
    "for data in trainData.data[:7500]:\n",
    "    im1 = torch.flatten(data[:,0:28])\n",
    "    im2 = torch.flatten(data[:,28:56])\n",
    "    im3 = torch.flatten(data[:,56:84])\n",
    "\n",
    "    hid1 = torch.matmul(im1,w01)\n",
    "    hid2 = torch.matmul(im2,w11)\n",
    "    hid3 = torch.matmul(im3,w21)\n",
    "    hidList.append(torch.vstack([hid1,hid2,hid3]))\n",
    "\n",
    "    hid1 = torch.matmul(im1,w01_dfa)\n",
    "    hid2 = torch.matmul(im2,w11_dfa)\n",
    "    hid3 = torch.matmul(im3,w21_dfa)\n",
    "    hidList_dfa.append(torch.vstack([hid1,hid2,hid3]))\n",
    "\n",
    "data = torch.vstack(hidList)\n",
    "data_dfa = torch.vstack(hidList_dfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, random_state=123, n_jobs=-1)\n",
    "z = tsne.fit_transform(data.numpy())\n",
    "z_dfa = tsne.fit_transform(data_dfa.numpy())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"comp-1\"] = z[:,0]\n",
    "df[\"comp-2\"] = z[:,1]\n",
    "df[\"comp-1_dfa\"] = z_dfa[:,0]\n",
    "df[\"comp-2_dfa\"] = z_dfa[:,1]\n",
    "\n",
    "df[\"y\"] = torch.hstack([trainData.left_target[:7500], trainData.middle_target[:7500], trainData.right_target[:7500]]).numpy()\n",
    "\n",
    "fig, (ax1, ax3) = plt.subplots(1, 2, figsize=(16,8))\n",
    "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n",
    "                palette=sns.color_palette(\"hls\", 10),\n",
    "                data=df, ax= ax1).set(title=\"BP\")\n",
    "ax1.axis(\"off\")\n",
    "ax1.legend().set_visible(False)\n",
    "\n",
    "sns.scatterplot(x=\"comp-1_dfa\", y=\"comp-2_dfa\", hue=df.y.tolist(),\n",
    "                palette=sns.color_palette(\"hls\", 10),\n",
    "                data=df, ax= ax3).set(title=\"DFA\")\n",
    "ax3.legend(loc = \"lower left\", bbox_to_anchor=(-0.24,0.0)) \n",
    "ax3.axis(\"off\")\n",
    "plt.suptitle(\"Hidden Representations\")\n",
    "plt.savefig(\"plots/hidden.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just need to change the labels to binary\n",
    "df[\"y\"] = torch.hstack([trainData.left_target[:7500]%2, trainData.middle_target[:7500]%2, trainData.right_target[:7500]%2]).numpy()\n",
    "\n",
    "fig, (ax1, ax3) = plt.subplots(1, 2, figsize=(16,8))\n",
    "sns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n",
    "                palette=sns.color_palette(\"hls\", 2),\n",
    "                data=df, ax= ax1).set(title=\"BP\")\n",
    "ax1.axis(\"off\")\n",
    "ax1.legend().set_visible(False)\n",
    "\n",
    "sns.scatterplot(x=\"comp-1_dfa\", y=\"comp-2_dfa\", hue=df.y.tolist(),\n",
    "                palette=sns.color_palette(\"hls\", 2),\n",
    "                data=df, ax= ax3).set(title=\"DFA\")\n",
    "\n",
    "ax3.legend(loc = \"lower left\", bbox_to_anchor=(-0.18,0.0))\n",
    "ax3.axis(\"off\") \n",
    "plt.suptitle(\"Hidden Representations\");"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sahibinden Veri Cekme Full.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "c684a6fc7ea5d844d0888e3fc402a914f9a0757a714c6fe2ccce21fe8443d9ca"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "423.991px",
    "width": "239.29px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
